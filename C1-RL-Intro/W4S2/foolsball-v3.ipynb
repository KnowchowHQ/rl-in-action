{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cv9hgPks9LhM"
   },
   "source": [
    "# Reinforcement learning with Foolsball\n",
    "- Reinforcement learning is learning to make decisions from experience.\n",
    "- Games are a good testbed for RL.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NWEnCb0m9ve8"
   },
   "source": [
    "# About Foolsball v3.0\n",
    "- 5x4 playground that provides a football/foosball-like environment.\n",
    "- A controllable player:\n",
    "  - always spawned in the top-left corner\n",
    "  - displayed as '‚öΩ'\n",
    "  - can move North, South, East or West.\n",
    "  - **Movements have some uncertainty associated with them.**\n",
    "  - can be controlled algorithmically\n",
    "- A number of **dynamic** opponents, each represented by üëï, that occupy certain locations on the field.\n",
    "- **The opponents can move up or down randomly independent of each other**\n",
    "- A goalpost ü•Ö that is fixed in the bottom right corner\n",
    "\n",
    "## Goals\n",
    "### Primary goal\n",
    "- We want the agent to learn to reach the goalpost \n",
    "\n",
    "### Secondary goals\n",
    "- We may want the agent to learn to be efficient in some sense, for example, take the shortest path to the goalpost. \n",
    "\n",
    "## Rules \n",
    "- Initial rules:\n",
    "    - **The ball can be (tried to be) moved in five ways: \\['n','e','w',s','x'\\], 'x' representing holding the ball in the current position.**\n",
    "    - **The environment is stochastic. So there's a small probability that a bad action gets triggered instead of the desired action.**\n",
    "    - Move the ball to an unmarked position: -1 points, game continues\n",
    "    - Move the ball to a position marked by a defender: -5 points, **game continues**\n",
    "    - Try to move the ball ouside the field: -1 (ball stays in the previous position), game continues\n",
    "    - Move the ball into the goal post position: +5, game terminates\n",
    "    - **Each opponent can randomly move up or down in its column**\n",
    "    - **The agent can only sense its state the presence or absense of defenders in the adjacent cells.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "agent = '‚öΩ'\n",
    "opponent = 'üëï'\n",
    "goal = 'ü•Ö'\n",
    "\n",
    "arena = [['‚öΩ', ' ' , 'üëï', ' ' ],\n",
    "         [' ' , ' ' , ' ' , 'üëï'],\n",
    "         [' ' , 'üëï', ' ' , ' ' ],\n",
    "         [' ' , ' ' , ' ' , 'üëï'],\n",
    "         [' ' , 'üëï', ' ' , 'ü•Ö']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key questions\n",
    "- What information should make up the state of the environment+agent now?\n",
    "- How and why should the state be encoded?\n",
    "- Is it possible to segregate next_state calculation and reward calculation as we have done in the past? \n",
    "- Think about the atomicity and order of the actions. Is agent's movement followed by defender movement equivalent to defneder movement followed by agents's movement?\n",
    "\n",
    "### Todos:\n",
    "- Implement the new rules in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "\n",
    "class Foolsball(object):\n",
    "    def __encode_indices__(self,row,col):\n",
    "        \"\"\"Convert from indices (row,col) to integer state.\"\"\"\n",
    "        return row*self.n_cols + col\n",
    "    \n",
    "    \n",
    "    def __decode_indices__(self, state):\n",
    "        \"\"\"Convert from integer state to indices(row,col)\"\"\"\n",
    "        row = state // self.n_cols\n",
    "        col = state % self.n_cols\n",
    "        return row,col\n",
    "        \n",
    "    def __make_observation__(self):\n",
    "        player_obs = self.state['agent']\n",
    "        \n",
    "        player_row, player_col = self.__decode_indices__(player_obs)\n",
    "        delta = ({'n':(-1,0), 'e':(0,+1), 'w':(0, -1), 's':(+1,0)})\n",
    "        \n",
    "        opponent_obs = {'n':None, 'e':None, 'w':None, 's':None} \n",
    "        for k in delta:\n",
    "            #Todo\n",
    "        \n",
    "        return(player_obs,opponent_obs)\n",
    "                \n",
    "            \n",
    "            \n",
    "        \n",
    "\n",
    "    def __deserialize__(self,map:list,agent:str,opponent:str, goal:str):\n",
    "        \"\"\"Convrt a string representation of a map into a 2D numpy array\n",
    "        Param map: list of lists of strings representing the player, opponents and goal.\n",
    "        Param agent: string representing the agent on the map \n",
    "        Param opponent: string representing every instance of an opponent player\n",
    "        Param goal: string representing the location of the goal on the map\n",
    "        \"\"\"\n",
    "        ## Capture dimensions and map.\n",
    "        self.n_rows = len(map)\n",
    "        self.n_cols = len(map[0])\n",
    "        self.map = np.asarray(map)\n",
    "\n",
    "        ## Store string representations for printing the map, etc.\n",
    "        self.agent_repr = agent\n",
    "        self.opponent_repr  = opponent\n",
    "        self.goal_repr = goal\n",
    "\n",
    "        ## Find initial state, the desired goal state and the state of the opponents. \n",
    "        player_state = None\n",
    "        goal_state = None\n",
    "        opponents_states = []\n",
    "\n",
    "        for row in range(self.n_rows):\n",
    "            for col in range(self.n_cols):\n",
    "                if map[row][col] == agent:\n",
    "                    player_state = self.__encode_indices__(row,col)\n",
    "                    self.map[row,col] = ' ' \n",
    "\n",
    "                elif map[row][col] == opponent:\n",
    "                    opponents_states.append(self.__encode_indices__(row,col))\n",
    "                    self.map[row,col] = ' ' \n",
    "\n",
    "                elif map[row][col] == goal:\n",
    "                    goal_state = self.__encode_indices__(row,col)\n",
    "        \n",
    "        self.init_state = {'agent':player_state,'opponents':opponents_states, 'goal':goal_state}\n",
    "       \n",
    "        \n",
    "        assert player_state is not None, f\"Map {map} does not specify an agent {agent} location\"\n",
    "        assert opponents_states is not None,  f\"Map {map} does not specify a goal {goal} location\"\n",
    "        assert goal_state,  f\"Map {map} does not specify any opponents {opponent} location\"\n",
    "\n",
    "        return self.init_state\n",
    "    \n",
    "    \n",
    "    def __init__(self,map,agent,opponent,goal,slip_prob):\n",
    "        \"\"\"Spawn the world, create variables to track state and actions.\"\"\"\n",
    "        # We need to track the location of the agent (the ball)\n",
    "        # and the opponents.\n",
    "        self.state = self.__deserialize__(map,agent,opponent,goal)\n",
    "        self.done = False\n",
    "        self.actions = ['n','e','w','s','x']\n",
    "        self.n_actions = len(self.actions)\n",
    "        self.slip_prob = slip_prob\n",
    "\n",
    "        # Set up the rewards\n",
    "        self.default_rewards = {'unmarked':-1, 'opponent':-5, 'outside':-1, 'goal':+5}\n",
    "        self.set_rewards(self.default_rewards)\n",
    "        \n",
    "    def set_rewards(self,rewards):\n",
    "        if not self.state == self.init_state:\n",
    "            print('Warning: Setting reward while not in initial state! You may want to call reset() first.')\n",
    "        for key in self.default_rewards:\n",
    "            assert key in rewards, f'Key {key} missing from reward.'\n",
    "        self.rewards = rewards\n",
    "            \n",
    "            \n",
    "    def reset(self):\n",
    "        \"\"\"Reset the environment to its initial state.\"\"\"\n",
    "        # There's really just two things we need to reset: the state, which should\n",
    "        # be reset to the initial state, and the `done` flag which should be \n",
    "        # cleared to signal that we are not in a terminal state anymore, even if we \n",
    "        # were earlier. \n",
    "        self.state = self.init_state\n",
    "        self.done  = False\n",
    "        return self.__make_observation__()\n",
    "    \n",
    "    \n",
    "    def step(self,action):\n",
    "        \"\"\"Simulate state transition based on current state and action received.\"\"\"\n",
    "        assert not self.done, \\\n",
    "        f'You cannot call step() in a terminal state({self.state}). Check the \"done\" flag before calling step() to avoid this.'\n",
    "        # Emulate stochastic behavior\n",
    "        actions = self.actions\n",
    "        selected_action_index = actions.index(action)\n",
    "        bad_action_prob = self.slip_prob/(len(actions)-1)\n",
    "        action_probs = np.ones(len(actions))* bad_action_prob\n",
    "        action_probs[selected_action_index] = 1-self.slip_prob\n",
    "        \n",
    "        executed_action = np.random.choice(actions,p=action_probs)\n",
    "        \n",
    "        \n",
    "        player_position, opponent_positions, goal_position =\\\n",
    "            self.state['agent'], self.state['opponents'], self.state['goal']\n",
    "        \n",
    "        action_to_index_delta = {'n':[-1,0], 'e':[0,+1], 'w':[0,-1], 's':[+1,0], 'x':[0,0]}\n",
    "        \n",
    "        # Simulate player's movement\n",
    "        row, col = self.__decode_indices__(player_position)\n",
    "        \n",
    "        row_delta, col_delta = action_to_index_delta[executed_action]\n",
    "        new_row , new_col = row+row_delta, col+col_delta\n",
    "\n",
    "        ## Check and advance player\n",
    "        if (0<=new_row<self.n_rows) and (0<=new_col<self.n_cols):\n",
    "            new_player_position = self.__encode_indices__(new_row, new_col) \n",
    "            outside = False\n",
    "        else:\n",
    "            new_player_position = player_position \n",
    "            outside = True\n",
    "\n",
    "        \n",
    "        # Simulate opponents' movements\n",
    "        new_opponent_positions = opponent_positions.copy()\n",
    "        for i in range(len(opponent_positions)):\n",
    "            row, col = self.__decode_indices__(opponent_positions[i])\n",
    "            random_action = np.random.choice(['n','s','x'])\n",
    "            \n",
    "            row_delta, col_delta = action_to_index_delta[random_action]\n",
    "            new_row , new_col = row+row_delta, col+col_delta\n",
    "            \n",
    "            if (0<=new_row<self.n_rows) and (0<=new_col<self.n_cols):\n",
    "                new_opponent_position = self.__encode_indices__(new_row, new_col)\n",
    "                \n",
    "                ### Check that there's no other opponent in the new position\n",
    "                ### and it is not a goal state either and update the position in \n",
    "                ### place\n",
    "                if not new_opponent_position in new_opponent_positions\\\n",
    "                and new_opponent_position is not goal_position:\n",
    "                    new_opponent_positions[i] = new_opponent_position\n",
    "                    \n",
    "        \n",
    "        # Calculate reward and done flags.\n",
    "        # The if conditions can overlap and reward needs to be accumulated.\n",
    "        # For example an (attempted) outside followed by a capture by an opponent.\n",
    "        reward = 0\n",
    "        done = False\n",
    "        normal_move = True\n",
    "        if new_player_position is  goal_position:\n",
    "            reward += self.rewards['goal']\n",
    "            done = True\n",
    "            normal_move = False\n",
    "            \n",
    "        if new_player_position in new_opponent_position:\n",
    "            reward += self.rewards['opponent']\n",
    "            normal_move = False\n",
    "        \n",
    "        if outside:\n",
    "            reward += self.rewards['outside']\n",
    "            normal_move = False\n",
    "        \n",
    "        if normal_move:\n",
    "            reward = self.rewards['unmarked']\n",
    "            \n",
    "\n",
    "        self.state = {'agent':new_player_position,'opponents':new_opponent_positions, 'goal':goal_position}\n",
    "        self.done = done\n",
    "        \n",
    "        observation = self.__make_observation__()\n",
    "        \n",
    "        return observation, reward, done\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    def render(self):\n",
    "        \"\"\"Pretty-print the environment and agent.\"\"\"\n",
    "        ## Create a copy of the map and change data type to accomodate\n",
    "        ## 3-character strings\n",
    "        _map = np.array(self.map, dtype='<U3')\n",
    "        \n",
    "        for opponent_position in self.state['opponents']:\n",
    "            opp_row, opp_col = self.__decode_indices__(opponent_position)\n",
    "            _map[opp_row,opp_col] = self.opponent_repr\n",
    "\n",
    "        ## Mark unoccupied positions with special symbol.\n",
    "        ## And add extra spacing to align all columns.\n",
    "        for row in range(_map.shape[0]):\n",
    "            for col in range(_map.shape[1]):\n",
    "                if _map[row,col] == ' ':\n",
    "                    _map[row,col] = ' + '\n",
    "\n",
    "                elif _map[row,col] == self.opponent_repr: \n",
    "                    _map[row,col] =  self.opponent_repr + ' '\n",
    "\n",
    "                elif _map[row,col] == self.goal_repr:\n",
    "                    _map[row,col] = ' ' + self.goal_repr + ' '\n",
    "\n",
    "        ## If current state overlaps with the goal state or one of the opponents'\n",
    "        ## states, susbstitute a distinct marker.\n",
    "        if self.state['agent'] == self.state['goal']:\n",
    "            r,c = self.__decode_indices__(self.state['agent'])\n",
    "            _map[r,c] = ' üèÅ '\n",
    "        elif self.state['agent'] in self.state['opponents']:\n",
    "            r,c = self.__decode_indices__(self.state['agent'])\n",
    "            _map[r,c] = ' ‚ùó '\n",
    "        else:\n",
    "            r,c = self.__decode_indices__(self.state['agent'])\n",
    "            _map[r,c] = ' ' + self.agent_repr\n",
    "\n",
    "        for row in range(_map.shape[0]):\n",
    "            for col in range(_map.shape[1]):\n",
    "                print(f' {_map[row,col]} ',end=\"\")\n",
    "            print('\\n') \n",
    "\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foolsball = Foolsball(arena, agent, opponent, goal, slip_prob=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5nn8RNR1NDZK"
   },
   "outputs": [],
   "source": [
    "foolsball.reset()\n",
    "foolsball.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interact with the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Move: n,s,e,w\n",
    "## Reset: r\n",
    "## Exit: x\n",
    "while True:\n",
    "    try:\n",
    "        act = input('>>')\n",
    "\n",
    "        if act in foolsball.actions:\n",
    "            print(foolsball.step(act))\n",
    "            print()\n",
    "            foolsball.render()\n",
    "        elif act == 'r':\n",
    "            print(foolsball.reset())\n",
    "            print()\n",
    "            foolsball.render()\n",
    "        elif act == 'q':\n",
    "            break\n",
    "        else:\n",
    "            print(f'Invalid input:{act}')\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Override the default reward structure.\n",
    "- Use a more sparse reward: {'unmarked':0, 'opponent':-5, 'outside':-1, 'goal':+5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Update reward structure to: {'unmarked':0, 'opponent':-5, 'outside':-1, 'goal':+5}\n",
    "foolsball.reset()\n",
    "foolsball.set_rewards({'unmarked':0, 'opponent':-5, 'outside':-1, 'goal':+5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement discounted returns¬∂\n",
    "$$Discounted\\ Return = R_{t_1} + \\gamma*R_{t_2} + \\gamma^2*R_{t_3} + ... + \\gamma^{n-1}*R_{t_n}$$where $R_{t_k}$ is the reward after step k and $\\gamma$ is called the discount factor.\n",
    "- Set the discount factor $\\gamma$ to 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discounted_return(path, gamma=0):\n",
    "    foolsball.reset()\n",
    "    foolsball.render()\n",
    "    _return_ = 0\n",
    "    discount_coeff = 1\n",
    "    for act in path: \n",
    "        next_state, reward, done = foolsball.step(act)\n",
    "        _return_ += discount_coeff*reward\n",
    "        discount_coeff *= gamma    \n",
    "\n",
    "        foolsball.render()\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    print(f'Return (accumulated reward): {_return_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HYPER_PARAMS = {'gamma':0.9}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_policy_from_returns_tbl(table):\n",
    "    policy = {s:None for s in table.index }\n",
    "    for state in table.index:\n",
    "        greedy_action = table.loc[state].idxmax()\n",
    "        policy[state] = greedy_action\n",
    "            \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_policy(policy):\n",
    "    direction_repr = {'n':' ü°ë ', 'e':' ü°í ', 'w':' ü°ê ', 's':' ü°ì ', None:' ‚¨§ '}\n",
    "\n",
    "    for row in range(foolsball.n_rows):\n",
    "        for col in range(foolsball.n_cols):\n",
    "            state = foolsball.__to_state__(row, col)\n",
    "            print(direction_repr[policy[state]],end='')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with incomplete Knowledge of the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def collect_random_episode():\n",
    "    state = foolsball.reset()\n",
    "    done = False\n",
    "    episode = []\n",
    "    \n",
    "    while not done:\n",
    "        action = np.random.choice(foolsball.actions)\n",
    "        next_state, reward, done = foolsball.step(action)\n",
    "        episode.append([state, action, reward])\n",
    "        state = next_state\n",
    "        \n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep = collect_random_episode()\n",
    "foolsball.render()\n",
    "print(ep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement discounted returns for episodes\n",
    "- If an episode is: (s1,a1,r1),(s2,a2,r2),(s3,a3,r3), (s4),  s4 being a terminal state:\n",
    "  - The (discounted) return for (s1,a1) is r1+Œ≥‚àór2+Œ≥2‚àór3\n",
    "  - The (discounted) return for (s2, a2)is r2+Œ≥‚àór3\n",
    "  - The (discounted) return for (s3,a3) is r3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_return_from_episode(ep, gamma=0):\n",
    "    states, actions, rewards = list(zip(*ep))\n",
    "    rewards = np.asarray(rewards)\n",
    "    discount_coeffs = np.asarray([np.power(gamma,p) for p in range(len(rewards))])\n",
    "    \n",
    "    l = len(rewards)\n",
    "    discounted_returns = [np.dot(rewards[i:],discount_coeffs[:l-i]) for i in range(l)]\n",
    "    \n",
    "    return (states, actions, discounted_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discounted_return_from_episode(ep, gamma=HYPER_PARAMS['gamma'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration-Exploitation with Epsilon Decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_epsilon_greedy_episode_from_returns_tbl(table, max_ep_len=20, epsilon=0.1):\n",
    "    state = foolsball.reset()\n",
    "    done = False\n",
    "    episode = []\n",
    "    \n",
    "    for _ in range(max_ep_len):\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "        actions = table.columns\n",
    "        action_probs = np.asarray([epsilon/len(actions)]*len(actions),dtype=np.float)\n",
    "\n",
    "        greedy_action_index = np.argmax(table.loc[state].values)\n",
    "        action_probs[greedy_action_index] += 1-epsilon\n",
    "\n",
    "        epsilon_greedy_action = np.random.choice(table.columns,p=action_probs)\n",
    "\n",
    "        next_state, reward, done = foolsball.step(epsilon_greedy_action)\n",
    "        episode.append([state, epsilon_greedy_action, reward])\n",
    "        state = next_state\n",
    "\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding the observations\n",
    "- Number of states\n",
    "- Encoding scheme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constant Alpha\n",
    "\n",
    "## The idea:\n",
    "- Dividing the accumulated returns by visit count has a non linear effect on the updates. (Go back to previous step and see for yourself).\n",
    "- Don't divide at all!\n",
    "- But we need to ensure that updates are small\n",
    "- Idea:\n",
    " - ESTIMATED_RETURNS_TBL.loc[s,a] and ret are both estimates of the same quantity.\n",
    " - Use the difference of the two estimates to update ESTIMATED_RETURNS_TBL.loc[s,a] much like we do in Deep Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ESTIMATED_RETURNS_TBL = pd.DataFrame.from_dict({s:{a:0 for a in foolsball.actions} for s in range(foolsball.n_states)}, orient='index')\n",
    "\n",
    "n_episodes = 5000\n",
    "epsilon = 1\n",
    "min_epsilon = 0.1\n",
    "epsilon_decay = 0.999\n",
    "\n",
    "alpha = 0.001\n",
    "\n",
    "for i in range(n_episodes):\n",
    "    estimated_returns = ESTIMATED_RETURNS_TBL\n",
    "  \n",
    "    epsilon = max(epsilon,min_epsilon)\n",
    "    episode_i = collect_epsilon_greedy_episode_from_returns_tbl(estimated_returns,epsilon=epsilon)\n",
    "    epsilon *= epsilon_decay\n",
    "    states, actions, discounted_returns = discounted_return_from_episode(episode_i, gamma=HYPER_PARAMS['gamma'])\n",
    "\n",
    "    for s,a,ret in zip(states, actions, discounted_returns):\n",
    "        ESTIMATED_RETURNS_TBL.loc[s,a] += alpha*(ret - ESTIMATED_RETURNS_TBL.loc[s,a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_returns = ESTIMATED_RETURNS_TBL\n",
    "print(estimated_returns)\n",
    "\n",
    "policy0 = greedy_policy_from_returns_tbl(estimated_returns)\n",
    "print(policy0)\n",
    "\n",
    "pretty_print_policy(policy0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How can we get faster convergence?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Try the SARSA and Q-learning appraches described [here](https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#sarsa-on-policy-td-control) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Difference(TD) Learning\n",
    " - Learn every step of an episode \n",
    " - This translates into using an updated policy for selecting the action at each step "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_action_from_Q(Q, state, epsilon):\n",
    "    actions = Q.columns\n",
    "    action_probs = np.asarray([epsilon/len(actions)]*len(actions),dtype=np.float)\n",
    "    \n",
    "    greedy_action_index = np.argmax(Q.loc[state].values)\n",
    "    action_probs[greedy_action_index] += 1-epsilon\n",
    "\n",
    "    epsilon_greedy_action = np.random.choice(Q.columns,p=action_probs)\n",
    "    \n",
    "    return epsilon_greedy_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARSA (State-Action-Reward-State-Action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "Q = pd.DataFrame.from_dict({s:{a:0 for a in foolsball.actions} for s in range(foolsball.n_states)}, orient='index')\n",
    "\n",
    "n_episodes = 2000\n",
    "epsilon = 1\n",
    "min_epsilon = 0.1\n",
    "epsilon_decay = 0.9995\n",
    "\n",
    "alpha = 0.01\n",
    "\n",
    "\n",
    "for i in tqdm(range(n_episodes)):\n",
    "    foolsball.reset()\n",
    "    s0 = foolsball.init_state\n",
    "    a0 = epsilon_greedy_action_from_Q(Q,s0,epsilon)\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        s1, reward, done  = foolsball.step(a0)\n",
    "        a1 = epsilon_greedy_action_from_Q(Q,s1,epsilon)\n",
    "        \n",
    "        Q.loc[s0,a0] += alpha*(reward + HYPER_PARAMS['gamma']*Q.loc[s1,a1] - Q.loc[s0,a0])\n",
    "        \n",
    "        s0, a0 = s1, a1\n",
    "  \n",
    "    epsilon *= epsilon_decay\n",
    "    epsilon = max(epsilon,min_epsilon)\n",
    "    \n",
    "    if (i+1)%500 == 0:\n",
    "        print(f'Iteration {i+1}')\n",
    "        policy = greedy_policy_from_returns_tbl(Q)\n",
    "        pretty_print_policy(policy)\n",
    "        \n",
    "\n",
    "policy_SARSA = greedy_policy_from_returns_tbl(Q)\n",
    "print(policy_SARSA)\n",
    "\n",
    "pretty_print_policy(policy_SARSA)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "Q = pd.DataFrame.from_dict({s:{a:0 for a in foolsball.actions} for s in range(foolsball.n_states)}, orient='index')\n",
    "\n",
    "n_episodes = 10000\n",
    "epsilon = 1\n",
    "min_epsilon = 0.1\n",
    "epsilon_decay = 0.9995\n",
    "\n",
    "alpha = 0.01\n",
    "rewards = np.zeros(n_episodes)\n",
    "\n",
    "\n",
    "for i in tqdm(range(n_episodes)):\n",
    "    foolsball.reset()\n",
    "    s0 = foolsball.init_state\n",
    "    done = False\n",
    "    \n",
    "    episode_reward = 0\n",
    "    while not done:\n",
    "        a0 = epsilon_greedy_action_from_Q(Q,s0,epsilon)\n",
    "        s1, reward, done  = foolsball.step(a0)\n",
    "        \n",
    "        Q.loc[s0,a0] += alpha*(reward + HYPER_PARAMS['gamma']*Q.loc[s1].max() - Q.loc[s0,a0])\n",
    "        episode_reward += reward\n",
    "        \n",
    "        s0 = s1\n",
    "  \n",
    "    epsilon *= epsilon_decay\n",
    "    epsilon = max(epsilon,min_epsilon)\n",
    "    \n",
    "    rewards[i] = episode_reward\n",
    "    \n",
    "    if (i+1)%500 == 0:\n",
    "        print(f'Iteration {i+1}')\n",
    "        policy = greedy_policy_from_returns_tbl(Q)\n",
    "        pretty_print_policy(policy)\n",
    "        #print(Q)\n",
    "        \n",
    "\n",
    "policy_Q_Learning = greedy_policy_from_returns_tbl(Q)\n",
    "print(policy_Q_Learning)\n",
    "\n",
    "pretty_print_policy(policy_Q_Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "windowed_rewards = np.convolve(rewards, np.ones(100), 'valid')\n",
    "plt.plot(windowed_rewards/100)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Rediscovering_RL_Notebook_0_SOLVED.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cv9hgPks9LhM"
   },
   "source": [
    "# Reinforcement learning with Foolsball\n",
    "- Reinforcement learning is learning to make decisions from experience.\n",
    "- Games are a good testbed for RL.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NWEnCb0m9ve8"
   },
   "source": [
    "# About Foolsball\n",
    "- 5x4 playground that provides a football/foosball-like environment.\n",
    "- A controllable player:\n",
    "  - always spawned in the top-left corner\n",
    "  - displayed as 'âš½'\n",
    "  - can move North, South, East or West.\n",
    "  - can be controlled algorithmically\n",
    "- A number of **static** opponents, each represented by ðŸ‘•, that occupy certain locations on the field.\n",
    "- A goalpost ðŸ¥… that is fixed in the bottom right corner\n",
    "\n",
    "## Goals\n",
    "### Primary goal\n",
    "- We want the agent to learn to reach the goalpost \n",
    "\n",
    "### Secondary goals\n",
    "- We may want the agent to learn to be efficient in some sense, for example, take the shortest path to the goalpost. \n",
    "\n",
    "## Rules \n",
    "- Initial rules:\n",
    "    - The ball can be (tried to be) moved in four direction: \\['n','e','w',s'\\]\n",
    "    - Move the ball to an unmarked position: -1 points\n",
    "    - Move the ball to a position marked by a defender: -5 points\n",
    "    - Try to move the ball ouside the field: -1 (ball stays in the previous position)\n",
    "    - Move the ball into the goal post position: +5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some variables definitions to make our enviroment look pretty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KcMMYrax7MVv"
   },
   "outputs": [],
   "source": [
    "agent = 'âš½'\n",
    "opponent = 'ðŸ‘•'\n",
    "goal = 'ðŸ¥…'\n",
    "\n",
    "arena = [['âš½', ' ' , 'ðŸ‘•', ' ' ],\n",
    "         [' ' , ' ' , ' ' , 'ðŸ‘•'],\n",
    "         [' ' , 'ðŸ‘•', ' ' , ' ' ],\n",
    "         [' ' , ' ' , ' ' , 'ðŸ‘•'],\n",
    "         [' ' , 'ðŸ‘•', ' ' , 'ðŸ¥…']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full code for the Foolsball class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Foolsball(object):\n",
    "    def __to_state__(self,row,col):\n",
    "        \"\"\"Convert from indices (row,col) to integer position.\"\"\"\n",
    "        return row*self.n_cols + col\n",
    "    \n",
    "    \n",
    "    def __to_indices__(self, state):\n",
    "        \"\"\"Convert from inteeger position to indices(row,col)\"\"\"\n",
    "        row = state // self.n_cols\n",
    "        col = state % self.n_cols\n",
    "        return row,col\n",
    "\n",
    "    def __deserialize__(self,map:list,agent:str,opponent:str, goal:str):\n",
    "        \"\"\"Convrt a string representation of a map into a 2D numpy array\n",
    "        Param map: list of lists of strings representing the player, opponents and goal.\n",
    "        Param agent: string representing the agent on the map \n",
    "        Param opponent: string representing every instance of an opponent player\n",
    "        Param goal: string representing the location of the goal on the map\n",
    "        \"\"\"\n",
    "        ## Capture dimensions and map.\n",
    "        self.n_rows = len(map)\n",
    "        self.n_cols = len(map[0])\n",
    "        self.n_states = self.n_rows * self.n_cols\n",
    "        self.map = np.asarray(map)\n",
    "\n",
    "        ## Store string representations for printing the map, etc.\n",
    "        self.agent_repr = agent\n",
    "        self.opponent_repr  = opponent\n",
    "        self.goal_repr = goal\n",
    "\n",
    "        ## Find initial state, the desired goal state and the state of the opponents. \n",
    "        self.init_state = None\n",
    "        self.goal_state = None\n",
    "        self.opponents_states = []\n",
    "\n",
    "        for row in range(self.n_rows):\n",
    "            for col in range(self.n_cols):\n",
    "                if map[row][col] == agent:\n",
    "                    # Store the initial state outside the map.\n",
    "                    # This helps in quickly resetting the game to the initial state and\n",
    "                    # also simplifies printing the map independent of the agent's state. \n",
    "                    self.init_state = self.__to_state__(row,col)\n",
    "                    self.map[row,col] = ' ' \n",
    "\n",
    "                elif map[row][col] == opponent:\n",
    "                    self.opponents_states.append(self.__to_state__(row,col))\n",
    "\n",
    "                elif map[row][col] == goal:\n",
    "                    self.goal_state = self.__to_state__(row,col)\n",
    "\n",
    "        assert self.init_state is not None, f\"Map {map} does not specify an agent {agent} location\"\n",
    "        assert self.goal_state is not None,  f\"Map {map} does not specify a goal {goal} location\"\n",
    "        assert self.opponents_states,  f\"Map {map} does not specify any opponents {opponent} location\"\n",
    "\n",
    "        return self.init_state\n",
    "    \n",
    "    \n",
    "    def __init__(self,map,agent,opponent,goal):\n",
    "        \"\"\"Spawn the world, create variables to track state and actions.\"\"\"\n",
    "        # We just need to track the location of the agent (the ball)\n",
    "        # Everything else is static and so a potential algorithm doesn't \n",
    "        # have to look at it. The variable `done` flags terminal states.\n",
    "        self.state = self.__deserialize__(map,agent,opponent,goal)\n",
    "        self.done = False\n",
    "        self.actions = ['n','e','w','s']\n",
    "\n",
    "        # Set up the rewards\n",
    "        self.default_rewards = {'unmarked':-1, 'opponent':-5, 'outside':-1, 'goal':+5}\n",
    "        self.set_rewards(self.default_rewards)\n",
    "        \n",
    "    def set_rewards(self,rewards):\n",
    "        if not self.state == self.init_state:\n",
    "            print('Warning: Setting reward while not in initial state! You may want to call reset() first.')\n",
    "        for key in self.default_rewards:\n",
    "            assert key in rewards, f'Key {key} missing from reward.'\n",
    "            self.rewards = rewards\n",
    "            \n",
    "            \n",
    "    def reset(self):\n",
    "        \"\"\"Reset the environment to its initial state.\"\"\"\n",
    "        # There's really just two things we need to reset: the state, which should\n",
    "        # be reset to the initial state, and the `done` flag which should be \n",
    "        # cleared to signal that we are not in a terminal state anymore, even if we \n",
    "        # were earlier. \n",
    "        self.state = self.init_state\n",
    "        self.done  = False\n",
    "        return self.state\n",
    "    \n",
    "    def __get_next_state_on_action__(self,state,action):\n",
    "        \"\"\"Return next state based on current state and action.\"\"\"\n",
    "        assert not self.__is_terminal_state__(state), f\"Action {action} undefined for terminal state {state}\"\n",
    "        \n",
    "        row, col = self.__to_indices__(state)\n",
    "        action_to_index_delta = {'n':[-1,0], 'e':[0,+1], 'w':[0,-1], 's':[+1,0]}\n",
    "\n",
    "        row_delta, col_delta = action_to_index_delta[action]\n",
    "        new_row , new_col = row+row_delta, col+col_delta\n",
    "\n",
    "        ## Return current state if next state is invalid\n",
    "        if not(0<=new_row<self.n_rows) or\\\n",
    "        not(0<=new_col<self.n_cols):\n",
    "            return state  \n",
    "\n",
    "        ## Construct state from new row and col and return it.    \n",
    "        return self.__to_state__(new_row, new_col)    \n",
    "    \n",
    "  \n",
    "    def __get_reward_for_transition__(self,state,next_state):\n",
    "        \"\"\" Return the reward based on the transition from current state to next state. \"\"\"\n",
    "        ## Transition rejected due to illegal action (move)\n",
    "        assert not self.__is_terminal_state__(state), f\"Reward is undefined for terminal state {state}\"\n",
    "        \n",
    "        if next_state == state:\n",
    "            reward = self.rewards['outside']\n",
    "\n",
    "        ## Goal!\n",
    "        elif next_state == self.goal_state:\n",
    "            reward = self.rewards['goal']\n",
    "\n",
    "        ## Ran into opponent. \n",
    "        elif next_state in self.opponents_states:\n",
    "            reward = self.rewards['opponent']\n",
    "\n",
    "        ## Made a safe and valid move.   \n",
    "        else:\n",
    "            reward = self.rewards['unmarked']\n",
    "\n",
    "        return reward    \n",
    "    \n",
    "    \n",
    "    def __is_terminal_state__(self, state):\n",
    "        return (state == self.goal_state) or (state in self.opponents_states) \n",
    "    \n",
    "      \n",
    "    def step(self,action):\n",
    "        \"\"\"Simulate state transition based on current state and action received.\"\"\"\n",
    "        assert not self.done, \\\n",
    "        f'You cannot call step() in a terminal state({self.state}). Check the \"done\" flag before calling step() to avoid this.'\n",
    "        next_state = self.__get_next_state_on_action__(self.state, action)\n",
    "\n",
    "        reward = self.__get_reward_for_transition__(self.state, next_state)\n",
    "\n",
    "        done = self.__is_terminal_state__(next_state)\n",
    "\n",
    "        self.state, self.done = next_state, done\n",
    "\n",
    "        return next_state, reward, done\n",
    "    \n",
    "    \n",
    "    \n",
    "    def render(self):\n",
    "        \"\"\"Pretty-print the environment and agent.\"\"\"\n",
    "        ## Create a copy of the map and change data type to accomodate\n",
    "        ## 3-character strings\n",
    "        _map = np.array(self.map, dtype='<U3')\n",
    "\n",
    "        ## Mark unoccupied positions with special symbol.\n",
    "        ## And add extra spacing to align all columns.\n",
    "        for row in range(_map.shape[0]):\n",
    "            for col in range(_map.shape[1]):\n",
    "                if _map[row,col] == ' ':\n",
    "                    _map[row,col] = ' + '\n",
    "\n",
    "                elif _map[row,col] == self.opponent_repr: \n",
    "                    _map[row,col] =  self.opponent_repr + ' '\n",
    "\n",
    "                elif _map[row,col] == self.goal_repr:\n",
    "                    _map[row,col] = ' ' + self.goal_repr + ' '\n",
    "\n",
    "        ## If current state overlaps with the goal state or one of the opponents'\n",
    "        ## states, susbstitute a distinct marker.\n",
    "        if self.state == self.goal_state:\n",
    "            r,c = self.__to_indices__(self.state)\n",
    "            _map[r,c] = ' ðŸ '\n",
    "        elif self.state in self.opponents_states:\n",
    "            r,c = self.__to_indices__(self.state)\n",
    "            _map[r,c] = ' â— '\n",
    "        else:\n",
    "            r,c = self.__to_indices__(self.state)\n",
    "            _map[r,c] = ' ' + self.agent_repr\n",
    "\n",
    "        for row in range(_map.shape[0]):\n",
    "            for col in range(_map.shape[1]):\n",
    "                print(f' {_map[row,col]} ',end=\"\")\n",
    "            print('\\n') \n",
    "\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MWXHSX9IWm9o"
   },
   "source": [
    "# Verify the environment\n",
    "Execute the two cell below and ensure that there are no runtime error and the rendering happens correctly. You should see output like this:\n",
    "\n",
    "```\n",
    "  âš½   +   ðŸ‘•    +  \n",
    "\n",
    "  +    +    +   ðŸ‘•  \n",
    "\n",
    "  +   ðŸ‘•    +    +  \n",
    "\n",
    "  +    +    +   ðŸ‘•  \n",
    "\n",
    "  +   ðŸ‘•    +    ðŸ¥…  \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hK5uEU1QGddR"
   },
   "outputs": [],
   "source": [
    "foolsball = Foolsball(arena, agent, opponent, goal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5nn8RNR1NDZK"
   },
   "outputs": [],
   "source": [
    "foolsball.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xjQffds2YGlj"
   },
   "source": [
    "# Explore the environment\n",
    "- Run the next cell to play with the environment and score a few goals. \n",
    "- If there are any errors you may want to go back and update the code for the final implementation of the `Foolsball` class. \n",
    "- Make sure to run the cell containing your final implementation before retrying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B4w1HCRHwG60"
   },
   "outputs": [],
   "source": [
    "## Move: n,s,e,w\n",
    "## Reset: r\n",
    "## Exit: x\n",
    "while True:\n",
    "    try:\n",
    "        act = input('>>')\n",
    "\n",
    "        if act in foolsball.actions:\n",
    "            print(foolsball.step(act))\n",
    "            print()\n",
    "            foolsball.render()\n",
    "        elif act == 'r':\n",
    "            print(foolsball.reset())\n",
    "            print()\n",
    "            foolsball.render()\n",
    "        elif act == 'x':\n",
    "            break\n",
    "        else:\n",
    "            print(f'Invalid input:{act}')\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fyjTlB4cCysH"
   },
   "source": [
    "# Understanding the first bits of terminology.\n",
    "## State \n",
    "- In RL, state refers to information about the environment and then agent.\n",
    "- An RL algorithm inspects the state to decide which action to take.\n",
    "- Exactly what information gets captured in `state` depends on a few factors:\n",
    "  - The complexity of the environment: \n",
    "    - The number of actors, \n",
    "    - the nature of the environment, for example text or images. \n",
    "  - The complexity of the algorithm\n",
    "    - A simple algorithm may only need information about the agent and its immediate surroundings.\n",
    "    - A more complex algorithm may need information about the whole environment.\n",
    "\n",
    "\n",
    "## Setup\n",
    "- In our case we want the algorithm to only know about the location of the agent on the field. \n",
    "- We could have included information about the opponents too which would perhaps aid in the decision making but we chose not to.  \n",
    "\n",
    "- The state therefore is a tuple: (row, col), representing the location of the agent. \n",
    "- There are 20 possible values that `state` can take on:\n",
    "  - `row` can range from 0 through 4\n",
    "  - `col` can range from 0 through 3\n",
    "\n",
    "## Implementation details\n",
    "- The state is actually stored as a single integer that can take on values between 0 and 19."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5auCn-_GLVeU"
   },
   "source": [
    "## Actions\n",
    "The agents can perfrom actions in an environment.\n",
    "\n",
    "## Setup\n",
    "- Our agent can perform one kind of action: navigate up, down, right or left.\n",
    "- It has 4 actions: 'n', 'e', 'w', 's'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qjxD0m0uMfbU"
   },
   "source": [
    "# Learning from experience\n",
    "Any RL set up can be modeled as shown below:\n",
    "\n",
    "![](https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcTMDmrmnl_dAyjCOErHPak2gLXmQTgQnVT8gQ&usqp=CAU)\n",
    "\n",
    "- The agent performs an action in the environment\n",
    "- The state of the environment and agent change as a result\n",
    "- The agent receives a reward and the updated state from the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ASC5S75zN8mO"
   },
   "source": [
    "## Rewards\n",
    "- Reward is the signal that an agent receives after it performs an action.\n",
    "- The reward structure has to be decided by us. \n",
    "- The biggest challenge of RL is that reward is often sparse. \n",
    "\n",
    "## Set up\n",
    "- In our case the reward depends on the rules of the game and our goal.\n",
    "  - If the agent runs into an opponent, the game gets over and the reward is negative (penalizes the agent).\n",
    "  - If the agent makes it to the goalpost, the game gets over and the reward is positive.\n",
    "  - if the agent takes the ball out of the field the reward is negative.\n",
    "  - If the agent makes a valid move what shoud the reward be?\n",
    "\n",
    "## Implementation\n",
    "- The default reward structure in our case is  `{'unmarked':-1, 'opponent':-5, 'outside':-1, 'goal':+5}`\n",
    "- This can be changed at any time by calling `set_rewards()`.\n",
    "- Taking the ball to an unmarked position seems like a small step towards reaching the goalpost. Why would we then ever want to have a negative reward for this type of manouver?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Understand the concept of returnsÂ¶\n",
    "- Complete the get_return() function.\n",
    "- Calculate returns for a few sample paths by running the next few cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = ['e','s','e','s','s','s','e']\n",
    "path2 = ['s','e','e','s','s','s','e']\n",
    "path3 = ['s','s','s','e','e','s','e']\n",
    "path4 = ['s','s','s','s','n','e','e','s','e']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_return(path):\n",
    "    foolsball.reset()\n",
    "    foolsball.render()\n",
    "  \n",
    "    _return_ = 0\n",
    "    for act in path: \n",
    "        #Todo\n",
    "\n",
    "    print(f'Return (accumulated reward): {_return_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_return(path1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_return(path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_return(path3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_return(path4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Experiment with a different reward structure.\n",
    "- Does it encourage the agent to take the shortest route?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Update reward structure to: {'unmarked':0, 'opponent':-5, 'outside':-1, 'goal':+5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_return(path1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_return(path4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Implement discounted returnsÂ¶\n",
    "- Get introduced to discounted return as a means to set acceptable time horizons.$$Discounted\\ Return = R_{t_1} + \\gamma*R_{t_2} + \\gamma^2*R_{t_3} + ... + \\gamma^{n-1}*R_{t_n}$$where $R_{t_k}$ is the reward after step k and $\\gamma$ is called the discount factor.\n",
    "- Complete the code below to implement discounted returns.\n",
    "- The discount factor $\\gamma$ is a hyperparameter (why?) often set to 0.9 ðŸ˜œ\n",
    "- Run the next few cells to see if discounting indeed has the effect we want (shorter paths have higher rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discounted_return(path, gamma=0):\n",
    "    foolsball.reset()\n",
    "    foolsball.render()\n",
    "    _return_ = 0\n",
    "            \n",
    "    print(f'Return (accumulated reward): {_return_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HYPER_PARAMS = {'gamma':0.9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_discounted_return(path1, HYPER_PARAMS['gamma'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_discounted_return(path4, HYPER_PARAMS['gamma'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formalizing the problem:\n",
    "- We want the agent to reach the goalpost AND attain the **highest discounted return**.\n",
    "- This means making safe and efficient moves\n",
    " - Running into opponents means game over\n",
    " - Repeated 'outsides' means inefficiency\n",
    " - Long detours are also inefficient\n",
    "\n",
    "## The Conundrum\n",
    " - We already know how to compute the discounted return for a single path.\n",
    " - We can generate all possible paths and calculate their returns and pick a path with the highest discounted return.\n",
    " - Alas there are too many paths (4 possible decisions at each step => combinatorial explosion).\n",
    " \n",
    "\n",
    "## The \"Trick\"\n",
    "- Even though there are too many paths, all of them are made up of a smaller number of (state,action) pairs.\n",
    "- We can calculate the highest discounted return for each of the 80(=20x4) state action pairs.\n",
    "- To emphasize, we want to calculate the return for each (state,action) pair, not just the reward.\n",
    " - Calculating return means peeking into the future, beyond that (state,action) pair.\n",
    "- Once we have the discounted returns for every (state, action) pair, we can construct a path with the highest discounted return by repeatedly picking the (state, action) pair that has the highest discounted return. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Construct a Rewards table\n",
    "- As a precursor to calculating returns for every (state,action) pair, let's try to calculate the reward for every (state,action) pair.\n",
    "\n",
    "- Understand how the code in the next two cells creates a Pandas table to store the rewards for every (state, action) pair.\n",
    "\n",
    "- We will cheat a little by using private methods of the Foolsball class\n",
    "\n",
    "- Use the __get_next_state_on_action__() and __get_reward_for_transition__() methods to complete the code in the third cell below.\n",
    "\n",
    "- Run the fourth cell to view the rewards table.\n",
    "\n",
    "- Notice that rewards for terminal states are kept undefined since no actions are allowed in those states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REWARDS_TBL = pd.DataFrame.from_dict({s:{a:None for a in foolsball.actions} for s in range(foolsball.n_states)}, orient='index')\n",
    "REWARDS_TBL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for state in REWARDS_TBL.index:\n",
    "    if not foolsball.__is_terminal_state__(state): #Only calculate rewards for non-terminal states\n",
    "        for action in REWARDS_TBL.columns:\n",
    "            next_state = foolsball.__get_next_state_on_action__(state,action)\n",
    "            REWARDS_TBL.loc[state, action] = foolsball.__get_reward_for_transition__(state, next_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminal_states = foolsball.opponents_states+[foolsball.goal_state]\n",
    "print(terminal_states)\n",
    "REWARDS_TBL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5:Construct an empty Returns table\n",
    "- Implement the function `make_returns_table()`\n",
    "\n",
    "- The table should have one row for every state and one column for every action, just like the rewards table.\n",
    "\n",
    "- The returns for all actions in every terminal state should be zero (why?)\n",
    "\n",
    "- The returns for all other state, action pairs should be left undefined (`None`)\n",
    "\n",
    "- Trying to fill up these entries will be the focus of the rest of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_returns_table(states_list, actions_list, terminal_states):\n",
    "    \"\"\"Create an empty returns table.\"\"\"\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RETURNS_TBL = make_returns_table(range(foolsball.n_states), foolsball.actions, terminal_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(terminal_states)\n",
    "RETURNS_TBL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Use dynamic programming to fill up the returns table\n",
    "- The **highest discounted return** for a **(state, action)** can be defined in terms of returns of the next state.\n",
    "\n",
    "$ Return(state_t,action_t) = Reward(state_t,state_{t+1}) + \\gamma * \\max \\begin{bmatrix} Return(state_{t+1}, action=='n')\\\\ Return(state_{t+1}, action=='e')\\\\  Return(state_{t+1}, action=='w')\\\\  Return(state_{t+1}, action=='s') \\end{bmatrix}$\n",
    "\n",
    "- Complete the implementation of `compute_returns_v0()` below \n",
    "- Run the next cell. The code causes a stack overflow. Why?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_returns_v0(table,state,action, debug=False): \n",
    "    \"\"\" Recursively compute the discounted return for a (state,action) pair\"\"\"\n",
    "    if table.loc[state, action] is None:\n",
    "        next_state = foolsball.__get_next_state_on_action__(state, action)\n",
    "        \n",
    "        if debug:\n",
    "            print(f'Trying to fill ({state},{action},{next_state})')\n",
    "\n",
    "    \n",
    "    else:\n",
    "        if debug:\n",
    "            print((state,action),f'Returning pre-computed return {table.loc[state][action]}')\n",
    "    \n",
    "    return table.loc[state,action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RETURNS_TBL = make_returns_table(range(foolsball.n_states), foolsball.actions, terminal_states)\n",
    "for a in foolsball.actions:\n",
    "    compute_returns_v0(RETURNS_TBL,state=0, action=a, debug=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contd..\n",
    "- The code above crashed becasue of indefinite recursion caused by a state,action pairs that resulted in the next state being the same as the current state\n",
    "- We can fix this by catching this case and a returning a large negative return.\n",
    "- Why is the large negative return necessary?\n",
    "- Make this change in the code below to implement this strategy\n",
    "- Run the next two cells to see if the RETURNS table gets filled up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_returns_v1(table,state,action, debug=False): \n",
    "    \"\"\" Recursively compute the discounted return for a (state,action) pair\"\"\"\n",
    "    if table.loc[state, action] is None:\n",
    "        next_state = foolsball.__get_next_state_on_action__(state, action)\n",
    "        \n",
    "        if debug:\n",
    "            print(f'Trying to fill ({state},{action},{next_state})')\n",
    "\n",
    "    \n",
    "    else:\n",
    "        if debug:\n",
    "            print((state,action),f'Returning pre-computed return {table.loc[state][action]}')\n",
    "    \n",
    "    return table.loc[state,action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RETURNS_TBL = make_returns_table(range(foolsball.n_states), foolsball.actions, terminal_states)\n",
    "for a in foolsball.actions:\n",
    "    compute_returns_v1(RETURNS_TBL,state=0, action=a, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contd..\n",
    "- The code above crashed becasue of indefinite mutual recursion caused by a (state,action) pair that resulted in the (next_state, next_action) bringing us back to the first state.\n",
    "- We can fix this by eliminating recursion altogether and relying on the values in the table. \n",
    "- How do we get values in the table to begin with? \n",
    "- How do we make sure that the returns for all (state, action) pairs get updated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_returns_v2(table,state,action, debug=False): \n",
    "    \"\"\" Recursively compute the discounted return for a (state,action) pair\"\"\"\n",
    "    if not foolsball.__is_terminal_state__(state):\n",
    "\n",
    "        next_state = foolsball.__get_next_state_on_action__(state, action)\n",
    "        reward = foolsball.__get_reward_for_transition__(state, next_state)\n",
    "    \n",
    "    return table.loc[state,action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_returns_table(states_list, actions_list, terminal_states):\n",
    "    \"\"\"Create an empty returns table.\"\"\"\n",
    "    table = pd.DataFrame.from_dict({s:{a:0 for a in actions_list} for s in states_list}, orient='index')\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RETURNS_TBL = make_returns_table(range(foolsball.n_states), foolsball.actions, terminal_states)\n",
    "RETURNS_TBL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in range(foolsball.n_states):\n",
    "    for a in foolsball.actions:\n",
    "        compute_returns_v2(RETURNS_TBL,state=s, action=a, debug=True)\n",
    "RETURNS_TBL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Let the returns stabilize (converge)Â¶\n",
    "- How do the estimates of returns evolve over iterations?\n",
    "- How do we know when to stop?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RETURNS_TBL = make_returns_table(range(foolsball.n_states), foolsball.actions, terminal_states)\n",
    "for i in range(1,50):\n",
    "    RETURNS_TBL_OLD = #Todo\n",
    "    for s in range(foolsball.n_states):\n",
    "        for a in foolsball.actions:\n",
    "            compute_returns_v2(RETURNS_TBL,state=s, action=a, debug=True)\n",
    "    \n",
    "    if i%5 == 0:\n",
    "        print(f'\\n{i} iterations')\n",
    "        print(RETURNS_TBL)\n",
    "    \n",
    "    deltas = #Todo\n",
    "    if #Todo < 1e-3:\n",
    "        print(f'\\nConvergence achieved at {i} iterations')\n",
    "        print(RETURNS_TBL)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8: Intro to Policies\n",
    "- To use the returns table for making decisions, we grab the action with the highest return for each of the states in the table.\n",
    "- This is called a greedy policy.\n",
    "- It's a greedy policy since we take the best action at each state.\n",
    "- Implement the function `greedy_policy_from_returns_tbl()`.\n",
    "- Run the next two cells to output a greedy policy derived from the returns table filled earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_policy_from_returns_tbl(table):\n",
    "    policy = {s:None for s in table.index }\n",
    "    #Todo\n",
    "            \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy0 = greedy_policy_from_returns_tbl(RETURNS_TBL)\n",
    "policy0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_policy(policy):\n",
    "    direction_repr = {'n':' ðŸ¡‘ ', 'e':' ðŸ¡’ ', 'w':' ðŸ¡ ', 's':' ðŸ¡“ ', None:' â¬¤ '}\n",
    "\n",
    "    for row in range(foolsball.n_rows):\n",
    "        for col in range(foolsball.n_cols):\n",
    "            state = foolsball.__to_state__(row, col)\n",
    "            print(direction_repr[policy[state]],end='')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print_policy(policy0)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Rediscovering_RL_Notebook_0_SOLVED.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cv9hgPks9LhM"
   },
   "source": [
    "# Reinforcement learning with Foolsball\n",
    "- Reinforcement learning is learning to make decisions from experience.\n",
    "- Games are a good testbed for RL.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NWEnCb0m9ve8"
   },
   "source": [
    "# About Foolsball\n",
    "- 5x4 playground that provides a football/foosball-like environment.\n",
    "- A controllable player:\n",
    "  - always spawned in the top-left corner\n",
    "  - displayed as '⚽'\n",
    "  - can move North, South, East or West.\n",
    "  - can be controlled algorithmically\n",
    "- A number of **static** opponents, each represented by 👕, that occupy certain locations on the field.\n",
    "- A goalpost 🥅 that is fixed in the bottom right corner\n",
    "\n",
    "## Goals\n",
    "### Primary goal\n",
    "- We want the agent to learn to reach the goalpost \n",
    "\n",
    "### Secondary goals\n",
    "- We may want the agent to learn to be efficient in some sense, for example, take the shortest path to the goalpost. \n",
    "\n",
    "## Rules \n",
    "- Initial rules:\n",
    "    - The ball can be (tried to be) moved in four direction: \\['n','e','w',s'\\]\n",
    "    - Move the ball to an unmarked position: -1 points\n",
    "    - Move the ball to a position marked by a defender: -5 points\n",
    "    - Try to move the ball ouside the field: -1 (ball stays in the previous position)\n",
    "    - Move the ball into the goal post position: +5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "agent = '⚽'\n",
    "opponent = '👕'\n",
    "goal = '🥅'\n",
    "\n",
    "arena = [['⚽', ' ' , '👕', ' ' ],\n",
    "         [' ' , ' ' , ' ' , '👕'],\n",
    "         [' ' , '👕', ' ' , ' ' ],\n",
    "         [' ' , ' ' , ' ' , '👕'],\n",
    "         [' ' , '👕', ' ' , '🥅']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Foolsball(object):\n",
    "    def __to_state__(self,row,col):\n",
    "        \"\"\"Convert from indices (row,col) to integer position.\"\"\"\n",
    "        return row*self.n_cols + col\n",
    "    \n",
    "    \n",
    "    def __to_indices__(self, state):\n",
    "        \"\"\"Convert from inteeger position to indices(row,col)\"\"\"\n",
    "        row = state // self.n_cols\n",
    "        col = state % self.n_cols\n",
    "        return row,col\n",
    "\n",
    "    def __deserialize__(self,map:list,agent:str,opponent:str, goal:str):\n",
    "        \"\"\"Convrt a string representation of a map into a 2D numpy array\n",
    "        Param map: list of lists of strings representing the player, opponents and goal.\n",
    "        Param agent: string representing the agent on the map \n",
    "        Param opponent: string representing every instance of an opponent player\n",
    "        Param goal: string representing the location of the goal on the map\n",
    "        \"\"\"\n",
    "        ## Capture dimensions and map.\n",
    "        self.n_rows = len(map)\n",
    "        self.n_cols = len(map[0])\n",
    "        self.n_states = self.n_rows * self.n_cols\n",
    "        self.map = np.asarray(map)\n",
    "\n",
    "        ## Store string representations for printing the map, etc.\n",
    "        self.agent_repr = agent\n",
    "        self.opponent_repr  = opponent\n",
    "        self.goal_repr = goal\n",
    "\n",
    "        ## Find initial state, the desired goal state and the state of the opponents. \n",
    "        self.init_state = None\n",
    "        self.goal_state = None\n",
    "        self.opponents_states = []\n",
    "\n",
    "        for row in range(self.n_rows):\n",
    "            for col in range(self.n_cols):\n",
    "                if map[row][col] == agent:\n",
    "                    # Store the initial state outside the map.\n",
    "                    # This helps in quickly resetting the game to the initial state and\n",
    "                    # also simplifies printing the map independent of the agent's state. \n",
    "                    self.init_state = self.__to_state__(row,col)\n",
    "                    self.map[row,col] = ' ' \n",
    "\n",
    "                elif map[row][col] == opponent:\n",
    "                    self.opponents_states.append(self.__to_state__(row,col))\n",
    "\n",
    "                elif map[row][col] == goal:\n",
    "                    self.goal_state = self.__to_state__(row,col)\n",
    "\n",
    "        assert self.init_state is not None, f\"Map {map} does not specify an agent {agent} location\"\n",
    "        assert self.goal_state is not None,  f\"Map {map} does not specify a goal {goal} location\"\n",
    "        assert self.opponents_states,  f\"Map {map} does not specify any opponents {opponent} location\"\n",
    "\n",
    "        return self.init_state\n",
    "    \n",
    "    \n",
    "    def __init__(self,map,agent,opponent,goal):\n",
    "        \"\"\"Spawn the world, create variables to track state and actions.\"\"\"\n",
    "        # We just need to track the location of the agent (the ball)\n",
    "        # Everything else is static and so a potential algorithm doesn't \n",
    "        # have to look at it. The variable `done` flags terminal states.\n",
    "        self.state = self.__deserialize__(map,agent,opponent,goal)\n",
    "        self.done = False\n",
    "        self.actions = ['n','e','w','s']\n",
    "\n",
    "        # Set up the rewards\n",
    "        self.default_rewards = {'unmarked':-1, 'opponent':-5, 'outside':-1, 'goal':+5}\n",
    "        self.set_rewards(self.default_rewards)\n",
    "        \n",
    "    def set_rewards(self,rewards):\n",
    "        if not self.state == self.init_state:\n",
    "            print('Warning: Setting reward while not in initial state! You may want to call reset() first.')\n",
    "        for key in self.default_rewards:\n",
    "            assert key in rewards, f'Key {key} missing from reward.'\n",
    "        self.rewards = rewards\n",
    "            \n",
    "            \n",
    "    def reset(self):\n",
    "        \"\"\"Reset the environment to its initial state.\"\"\"\n",
    "        # There's really just two things we need to reset: the state, which should\n",
    "        # be reset to the initial state, and the `done` flag which should be \n",
    "        # cleared to signal that we are not in a terminal state anymore, even if we \n",
    "        # were earlier. \n",
    "        self.state = self.init_state\n",
    "        self.done  = False\n",
    "        return self.state\n",
    "    \n",
    "    def __get_next_state_on_action__(self,state,action):\n",
    "        \"\"\"Return next state based on current state and action.\"\"\"\n",
    "        assert not self.__is_terminal_state__(state), f\"Action {action} undefined for terminal state {state}\"\n",
    "        \n",
    "        row, col = self.__to_indices__(state)\n",
    "        action_to_index_delta = {'n':[-1,0], 'e':[0,+1], 'w':[0,-1], 's':[+1,0]}\n",
    "\n",
    "        row_delta, col_delta = action_to_index_delta[action]\n",
    "        new_row , new_col = row+row_delta, col+col_delta\n",
    "\n",
    "        ## Return current state if next state is invalid\n",
    "        if not(0<=new_row<self.n_rows) or\\\n",
    "        not(0<=new_col<self.n_cols):\n",
    "            return state  \n",
    "\n",
    "        ## Construct state from new row and col and return it.    \n",
    "        return self.__to_state__(new_row, new_col)    \n",
    "    \n",
    "  \n",
    "    def __get_reward_for_transition__(self,state,next_state):\n",
    "        \"\"\" Return the reward based on the transition from current state to next state. \"\"\"\n",
    "        ## Transition rejected due to illegal action (move)\n",
    "        assert not self.__is_terminal_state__(state), f\"Reward is undefined for terminal state {state}\"\n",
    "        \n",
    "        if next_state == state:\n",
    "            reward = self.rewards['outside']\n",
    "\n",
    "        ## Goal!\n",
    "        elif next_state == self.goal_state:\n",
    "            reward = self.rewards['goal']\n",
    "\n",
    "        ## Ran into opponent. \n",
    "        elif next_state in self.opponents_states:\n",
    "            reward = self.rewards['opponent']\n",
    "\n",
    "        ## Made a safe and valid move.   \n",
    "        else:\n",
    "            reward = self.rewards['unmarked']\n",
    "\n",
    "        return reward    \n",
    "    \n",
    "    \n",
    "    def __is_terminal_state__(self, state):\n",
    "        return (state == self.goal_state) or (state in self.opponents_states) \n",
    "    \n",
    "      \n",
    "    def step(self,action):\n",
    "        \"\"\"Simulate state transition based on current state and action received.\"\"\"\n",
    "        assert not self.done, \\\n",
    "        f'You cannot call step() in a terminal state({self.state}). Check the \"done\" flag before calling step() to avoid this.'\n",
    "        next_state = self.__get_next_state_on_action__(self.state, action)\n",
    "\n",
    "        reward = self.__get_reward_for_transition__(self.state, next_state)\n",
    "\n",
    "        done = self.__is_terminal_state__(next_state)\n",
    "\n",
    "        self.state, self.done = next_state, done\n",
    "\n",
    "        return next_state, reward, done\n",
    "    \n",
    "    \n",
    "    \n",
    "    def render(self):\n",
    "        \"\"\"Pretty-print the environment and agent.\"\"\"\n",
    "        ## Create a copy of the map and change data type to accomodate\n",
    "        ## 3-character strings\n",
    "        _map = np.array(self.map, dtype='<U3')\n",
    "\n",
    "        ## Mark unoccupied positions with special symbol.\n",
    "        ## And add extra spacing to align all columns.\n",
    "        for row in range(_map.shape[0]):\n",
    "            for col in range(_map.shape[1]):\n",
    "                if _map[row,col] == ' ':\n",
    "                    _map[row,col] = ' + '\n",
    "\n",
    "                elif _map[row,col] == self.opponent_repr: \n",
    "                    _map[row,col] =  self.opponent_repr + ' '\n",
    "\n",
    "                elif _map[row,col] == self.goal_repr:\n",
    "                    _map[row,col] = ' ' + self.goal_repr + ' '\n",
    "\n",
    "        ## If current state overlaps with the goal state or one of the opponents'\n",
    "        ## states, susbstitute a distinct marker.\n",
    "        if self.state == self.goal_state:\n",
    "            r,c = self.__to_indices__(self.state)\n",
    "            _map[r,c] = ' 🏁 '\n",
    "        elif self.state in self.opponents_states:\n",
    "            r,c = self.__to_indices__(self.state)\n",
    "            _map[r,c] = ' ❗ '\n",
    "        else:\n",
    "            r,c = self.__to_indices__(self.state)\n",
    "            _map[r,c] = ' ' + self.agent_repr\n",
    "\n",
    "        for row in range(_map.shape[0]):\n",
    "            for col in range(_map.shape[1]):\n",
    "                print(f' {_map[row,col]} ',end=\"\")\n",
    "            print('\\n') \n",
    "\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "foolsball = Foolsball(arena, agent, opponent, goal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5nn8RNR1NDZK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ⚽   +   👕    +  \n",
      "\n",
      "  +    +    +   👕  \n",
      "\n",
      "  +   👕    +    +  \n",
      "\n",
      "  +    +    +   👕  \n",
      "\n",
      "  +   👕    +    🥅  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "foolsball.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Override the default reward structure.\n",
    "- Use a more sparse reward: {'unmarked':0, 'opponent':-5, 'outside':-1, 'goal':+5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Update reward structure to: {'unmarked':0, 'opponent':-5, 'outside':-1, 'goal':+5}\n",
    "foolsball.reset()\n",
    "foolsball.set_rewards({'unmarked':0, 'opponent':-5, 'outside':-1, 'goal':+5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement discounted returns¶\n",
    "$$Discounted\\ Return = R_{t_1} + \\gamma*R_{t_2} + \\gamma^2*R_{t_3} + ... + \\gamma^{n-1}*R_{t_n}$$where $R_{t_k}$ is the reward after step k and $\\gamma$ is called the discount factor.\n",
    "- Set the discount factor $\\gamma$ to 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discounted_return(path, gamma=0):\n",
    "    foolsball.reset()\n",
    "    foolsball.render()\n",
    "    _return_ = 0\n",
    "    discount_coeff = 1\n",
    "    for act in path: \n",
    "        next_state, reward, done = foolsball.step(act)\n",
    "        _return_ += discount_coeff*reward\n",
    "        discount_coeff *= gamma    \n",
    "\n",
    "        foolsball.render()\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    print(f'Return (accumulated reward): {_return_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "HYPER_PARAMS = {'gamma':0.9}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use dynamic programming to fill up the returns table\n",
    "- The **highest discounted return** for a **(state, action)** can be defined in terms of returns of the next state.\n",
    "\n",
    "$ Return(state_t,action_t) = Reward(state_t,state_{t+1}) + \\gamma * \\max \\begin{bmatrix} Return(state_{t+1}, action_{t+1}=='n')\\\\ Return(state_{t+1}, action_{t+1}=='e')\\\\  Return(state_{t+1}, action_{t+1}=='w')\\\\  Return(state_{t+1}, action_{t+1}=='s') \\end{bmatrix}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def make_returns_table(states_list, actions_list, terminal_states):\n",
    "    \"\"\"Create an empty returns table where each entry is initialized arbitrarily.\"\"\"\n",
    "    table = pd.DataFrame.from_dict({s:{a:0 for a in actions_list} for s in states_list}, orient='index')\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>e</th>\n",
       "      <th>w</th>\n",
       "      <th>s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    n  e  w  s\n",
       "0   0  0  0  0\n",
       "1   0  0  0  0\n",
       "2   0  0  0  0\n",
       "3   0  0  0  0\n",
       "4   0  0  0  0\n",
       "5   0  0  0  0\n",
       "6   0  0  0  0\n",
       "7   0  0  0  0\n",
       "8   0  0  0  0\n",
       "9   0  0  0  0\n",
       "10  0  0  0  0\n",
       "11  0  0  0  0\n",
       "12  0  0  0  0\n",
       "13  0  0  0  0\n",
       "14  0  0  0  0\n",
       "15  0  0  0  0\n",
       "16  0  0  0  0\n",
       "17  0  0  0  0\n",
       "18  0  0  0  0\n",
       "19  0  0  0  0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terminal_states = foolsball.opponents_states + [foolsball.goal_state]\n",
    "RETURNS_TBL = make_returns_table(range(foolsball.n_states), foolsball.actions, terminal_states)\n",
    "RETURNS_TBL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_returns(table,state,action, debug=False): \n",
    "    \"\"\" Recursively compute the discounted return for a (state,action) pair\"\"\"\n",
    "    if not foolsball.__is_terminal_state__(state):\n",
    "\n",
    "        next_state = foolsball.__get_next_state_on_action__(state, action)\n",
    "        reward = foolsball.__get_reward_for_transition__(state, next_state)\n",
    "\n",
    "        update = HYPER_PARAMS['gamma'] *\\\n",
    "        max(table.loc[next_state, foolsball.actions[0]],\\\n",
    "        table.loc[next_state, foolsball.actions[1]],\\\n",
    "        table.loc[next_state, foolsball.actions[2]],\\\n",
    "        table.loc[next_state, foolsball.actions[3]])\n",
    "\n",
    "        table.loc[state, action]  = reward + update\n",
    "    \n",
    "    return table.loc[state,action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>e</th>\n",
       "      <th>w</th>\n",
       "      <th>s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-5.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-5.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      n    e    w    s\n",
       "0  -1.0  0.0 -1.0  0.0\n",
       "1  -1.0 -5.0  0.0  0.0\n",
       "2   0.0  0.0  0.0  0.0\n",
       "3  -1.0 -1.0 -5.0 -5.0\n",
       "4   0.0  0.0 -1.0  0.0\n",
       "5   0.0  0.0  0.0 -5.0\n",
       "6  -5.0 -5.0  0.0  0.0\n",
       "7   0.0  0.0  0.0  0.0\n",
       "8   0.0 -5.0 -1.0  0.0\n",
       "9   0.0  0.0  0.0  0.0\n",
       "10  0.0  0.0 -5.0  0.0\n",
       "11 -5.0 -1.0  0.0 -5.0\n",
       "12  0.0  0.0 -1.0  0.0\n",
       "13 -5.0  0.0  0.0 -5.0\n",
       "14  0.0 -5.0  0.0  0.0\n",
       "15  0.0  0.0  0.0  0.0\n",
       "16  0.0 -5.0 -1.0 -1.0\n",
       "17  0.0  0.0  0.0  0.0\n",
       "18  0.0  5.0 -5.0  3.5\n",
       "19  0.0  0.0  0.0  0.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for s in range(foolsball.n_states):\n",
    "    for a in foolsball.actions:\n",
    "        compute_returns(RETURNS_TBL,state=s, action=a, debug=True)\n",
    "RETURNS_TBL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let the returns stabilize (converge)¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 iterations\n",
      "          n       e        w        s\n",
      "0  -1.00000  0.0000 -1.00000  0.00000\n",
      "1  -1.00000 -5.0000  0.00000  0.00000\n",
      "2   0.00000  0.0000  0.00000  0.00000\n",
      "3  -4.09510 -4.0951 -5.00000 -5.00000\n",
      "4   0.00000  0.0000 -1.00000  0.00000\n",
      "5   0.00000  3.2805  0.00000 -5.00000\n",
      "6  -5.00000 -5.0000  2.95245  3.64500\n",
      "7   0.00000  0.0000  0.00000  0.00000\n",
      "8   0.00000 -5.0000 -1.00000  3.28050\n",
      "9   0.00000  0.0000  0.00000  0.00000\n",
      "10  3.28050  3.2805 -5.00000  4.05000\n",
      "11 -5.00000  2.2805  3.64500 -5.00000\n",
      "12  2.95245  3.6450  2.28050  2.95245\n",
      "13 -5.00000  4.0500  3.28050 -5.00000\n",
      "14  3.64500 -5.0000  3.64500  4.50000\n",
      "15  0.00000  0.0000  0.00000  0.00000\n",
      "16  3.28050 -5.0000  1.95245  1.95245\n",
      "17  0.00000  0.0000  0.00000  0.00000\n",
      "18  4.05000  5.0000 -5.00000  3.50000\n",
      "19  0.00000  0.0000  0.00000  0.00000\n",
      "\n",
      "Convergence achieved at 9 iterations\n",
      "           n         e         w         s\n",
      "0   1.391485  2.657205  1.391485  2.657205\n",
      "1   1.657205 -5.000000  2.391485  2.952450\n",
      "2   0.000000  0.000000  0.000000  0.000000\n",
      "3  -5.500000 -5.500000 -5.000000 -5.000000\n",
      "4   2.391485  2.952450  1.657205  2.952450\n",
      "5   2.657205  3.280500  2.657205 -5.000000\n",
      "6  -5.000000 -5.000000  2.952450  3.645000\n",
      "7   0.000000  0.000000  0.000000  0.000000\n",
      "8   2.657205 -5.000000  1.952450  3.280500\n",
      "9   0.000000  0.000000  0.000000  0.000000\n",
      "10  3.280500  3.280500 -5.000000  4.050000\n",
      "11 -5.000000  2.280500  3.645000 -5.000000\n",
      "12  2.952450  3.645000  2.280500  2.952450\n",
      "13 -5.000000  4.050000  3.280500 -5.000000\n",
      "14  3.645000 -5.000000  3.645000  4.500000\n",
      "15  0.000000  0.000000  0.000000  0.000000\n",
      "16  3.280500 -5.000000  1.952450  1.952450\n",
      "17  0.000000  0.000000  0.000000  0.000000\n",
      "18  4.050000  5.000000 -5.000000  3.500000\n",
      "19  0.000000  0.000000  0.000000  0.000000\n"
     ]
    }
   ],
   "source": [
    "RETURNS_TBL = make_returns_table(range(foolsball.n_states), foolsball.actions, terminal_states)\n",
    "for i in range(1,50):\n",
    "    RETURNS_TBL_OLD = RETURNS_TBL.copy()\n",
    "    for s in range(foolsball.n_states):\n",
    "        for a in foolsball.actions:\n",
    "            compute_returns(RETURNS_TBL,state=s, action=a, debug=True)\n",
    "    \n",
    "    if i%5 == 0:\n",
    "        print(f'\\n{i} iterations')\n",
    "        print(RETURNS_TBL)\n",
    "    \n",
    "    deltas = RETURNS_TBL- RETURNS_TBL_OLD\n",
    "    if abs(deltas.values.max()) < 1e-3:\n",
    "        print(f'\\nConvergence achieved at {i} iterations')\n",
    "        print(RETURNS_TBL)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_policy_from_returns_tbl(table):\n",
    "    policy = {s:None for s in table.index }\n",
    "    for state in table.index:\n",
    "        if state not in terminal_states:\n",
    "            greedy_action = table.loc[state].idxmax()\n",
    "            policy[state] = greedy_action\n",
    "            \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'e',\n",
       " 1: 's',\n",
       " 2: None,\n",
       " 3: 'w',\n",
       " 4: 'e',\n",
       " 5: 'e',\n",
       " 6: 's',\n",
       " 7: None,\n",
       " 8: 's',\n",
       " 9: None,\n",
       " 10: 's',\n",
       " 11: 'w',\n",
       " 12: 'e',\n",
       " 13: 'e',\n",
       " 14: 's',\n",
       " 15: None,\n",
       " 16: 'n',\n",
       " 17: None,\n",
       " 18: 'e',\n",
       " 19: None}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy0 = greedy_policy_from_returns_tbl(RETURNS_TBL)\n",
    "policy0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_policy(policy):\n",
    "    direction_repr = {'n':' 🡑 ', 'e':' 🡒 ', 'w':' 🡐 ', 's':' 🡓 ', None:' ⬤ '}\n",
    "\n",
    "    for row in range(foolsball.n_rows):\n",
    "        for col in range(foolsball.n_cols):\n",
    "            state = foolsball.__to_state__(row, col)\n",
    "            print(direction_repr[policy[state]],end='')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 🡒  🡓  ⬤  🡐 \n",
      " 🡒  🡒  🡓  ⬤ \n",
      " 🡓  ⬤  🡓  🡐 \n",
      " 🡒  🡒  🡓  ⬤ \n",
      " 🡑  ⬤  🡒  ⬤ \n"
     ]
    }
   ],
   "source": [
    "pretty_print_policy(policy0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with incomplete Knowledge of the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def collect_random_episode():\n",
    "    state = foolsball.reset()\n",
    "    done = False\n",
    "    episode = []\n",
    "    \n",
    "    while not done:\n",
    "        action = np.random.choice(foolsball.actions)\n",
    "        next_state, reward, done = foolsball.step(action)\n",
    "        episode.append([state, action, reward])\n",
    "        state = next_state\n",
    "        \n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  +    +    ❗    +  \n",
      "\n",
      "  +    +    +   👕  \n",
      "\n",
      "  +   👕    +    +  \n",
      "\n",
      "  +    +    +   👕  \n",
      "\n",
      "  +   👕    +    🥅  \n",
      "\n",
      "\n",
      "[[0, 'w', -1], [0, 'w', -1], [0, 'w', -1], [0, 's', 0], [4, 'e', 0], [5, 'n', 0], [1, 's', 0], [5, 'n', 0], [1, 'w', 0], [0, 's', 0], [4, 'e', 0], [5, 'e', 0], [6, 'n', -5]]\n"
     ]
    }
   ],
   "source": [
    "ep = collect_random_episode()\n",
    "foolsball.render()\n",
    "print(ep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement discounted returns for episodes\n",
    "- If an episode is: (s1,a1,r1),(s2,a2,r2),(s3,a3,r3), (s4),  s4 being a terminal state:\n",
    "  - The (discounted) return for (s1,a1) is r1+γ∗r2+γ2∗r3\n",
    "  - The (discounted) return for (s2, a2)is r2+γ∗r3\n",
    "  - The (discounted) return for (s3,a3) is r3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_return_from_episode(ep, gamma=0):\n",
    "    states, actions, rewards = list(zip(*ep))\n",
    "    rewards = np.asarray(rewards)\n",
    "    discount_coeffs = np.asarray([np.power(gamma,p) for p in range(len(rewards))])\n",
    "    \n",
    "    l = len(rewards)\n",
    "    discounted_returns = [np.dot(rewards[i:],discount_coeffs[:l-i]) for i in range(l)]\n",
    "    \n",
    "    return (states, actions, discounted_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0, 0, 0, 0, 4, 5, 1, 5, 1, 0, 4, 5, 6),\n",
       " ('w', 'w', 'w', 's', 'e', 'n', 's', 'n', 'w', 's', 'e', 'e', 'n'),\n",
       " [-4.122147682405,\n",
       "  -3.4690529804500003,\n",
       "  -2.7433922005000007,\n",
       "  -1.9371024450000005,\n",
       "  -2.1523360500000006,\n",
       "  -2.3914845000000002,\n",
       "  -2.6572050000000003,\n",
       "  -2.9524500000000002,\n",
       "  -3.2805,\n",
       "  -3.6450000000000005,\n",
       "  -4.050000000000001,\n",
       "  -4.5,\n",
       "  -5.0])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discounted_return_from_episode(ep, gamma=HYPER_PARAMS['gamma'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration-Exploitation with Epsilon Decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_epsilon_greedy_episode_from_returns_tbl(table, max_ep_len=20, epsilon=0.1):\n",
    "    state = foolsball.reset()\n",
    "    done = False\n",
    "    episode = []\n",
    "    \n",
    "    for _ in range(max_ep_len):\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "        actions = table.columns\n",
    "        action_probs = np.asarray([epsilon/len(actions)]*len(actions),dtype=np.float)\n",
    "\n",
    "        greedy_action_index = np.argmax(table.loc[state].values)\n",
    "        action_probs[greedy_action_index] += 1-epsilon\n",
    "\n",
    "        epsilon_greedy_action = np.random.choice(table.columns,p=action_probs)\n",
    "\n",
    "        next_state, reward, done = foolsball.step(epsilon_greedy_action)\n",
    "        episode.append([state, epsilon_greedy_action, reward])\n",
    "        state = next_state\n",
    "\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ESTIMATED_RETURNS_TBL = pd.DataFrame.from_dict({s:{a:0 for a in foolsball.actions} for s in range(foolsball.n_states)}, orient='index')\n",
    "VISITS_COUNTS_TBL = pd.DataFrame.from_dict({s:{a:0 for a in foolsball.actions} for s in range(foolsball.n_states)}, orient='index')\n",
    "\n",
    "n_episodes = 5000\n",
    "epsilon = 1\n",
    "min_epsilon = 0.1\n",
    "epsilon_decay = 0.999\n",
    "\n",
    "for i in range(n_episodes):\n",
    "    estimated_returns = ESTIMATED_RETURNS_TBL.div(VISITS_COUNTS_TBL+1)\n",
    "  \n",
    "    epsilon = max(epsilon, min_epsilon)\n",
    "    episode_i = collect_epsilon_greedy_episode_from_returns_tbl(estimated_returns,epsilon=epsilon)\n",
    "    epsilon *= epsilon_decay\n",
    "    #print(episode_i)\n",
    "    states, actions, discounted_returns = discounted_return_from_episode(episode_i, gamma=HYPER_PARAMS['gamma'])\n",
    "\n",
    "    for s,a,ret in zip(states, actions, discounted_returns):\n",
    "        ESTIMATED_RETURNS_TBL.loc[s,a] += ret\n",
    "        VISITS_COUNTS_TBL.loc[s,a] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           n         e         w         s\n",
      "0  -2.113456 -0.810968 -2.093475 -0.261176\n",
      "1  -3.320869 -4.973262 -2.426967 -0.244609\n",
      "2   0.000000  0.000000  0.000000  0.000000\n",
      "3   0.000000  0.000000  0.000000  0.000000\n",
      "4  -0.258033 -0.185558 -1.828671 -1.042799\n",
      "5  -1.892051  0.635241 -1.867702 -4.980989\n",
      "6  -4.958678 -4.954955 -1.380574  1.553308\n",
      "7   0.000000  0.000000  0.000000  0.000000\n",
      "8  -0.653853 -4.966667 -3.097294 -0.932541\n",
      "9   0.000000  0.000000  0.000000  0.000000\n",
      "10 -0.731833 -0.603854 -4.959016  2.538875\n",
      "11 -4.444444 -3.726450  0.980481 -4.666667\n",
      "12 -2.742345 -0.477179 -2.663998 -2.468393\n",
      "13 -4.666667  1.662853 -2.306547 -4.642857\n",
      "14 -0.419025 -4.935897  0.817228  3.558155\n",
      "15  0.000000  0.000000  0.000000  0.000000\n",
      "16 -1.167428 -4.500000 -3.554386 -4.732148\n",
      "17  0.000000  0.000000  0.000000  0.000000\n",
      "18  2.137601  4.995726 -4.929577  2.376720\n",
      "19  0.000000  0.000000  0.000000  0.000000\n",
      "{0: 's', 1: 's', 2: None, 3: 'n', 4: 'e', 5: 'e', 6: 's', 7: None, 8: 'n', 9: None, 10: 's', 11: 'w', 12: 'e', 13: 'e', 14: 's', 15: None, 16: 'n', 17: None, 18: 'e', 19: None}\n",
      " 🡓  🡓  ⬤  🡑 \n",
      " 🡒  🡒  🡓  ⬤ \n",
      " 🡑  ⬤  🡓  🡐 \n",
      " 🡒  🡒  🡓  ⬤ \n",
      " 🡑  ⬤  🡒  ⬤ \n"
     ]
    }
   ],
   "source": [
    "estimated_returns = ESTIMATED_RETURNS_TBL.div(VISITS_COUNTS_TBL+1) ## Averaging returns. Avoid dividing by zeros.\n",
    "print(estimated_returns)\n",
    "\n",
    "policy4 = greedy_policy_from_returns_tbl(estimated_returns)\n",
    "print(policy4)\n",
    "\n",
    "pretty_print_policy(policy4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constant Alpha\n",
    "\n",
    "## The idea:\n",
    "- Dividing the accumulated returns by visit count has a non linear effect on the updates. (Go back to previous step and see for yourself).\n",
    "- Don't divide at all!\n",
    "- But we need to ensure that updates are small\n",
    "- Idea:\n",
    " - ESTIMATED_RETURNS_TBL.loc[s,a] and ret are both estimates of the same quantity.\n",
    " - Use the difference of the two estimates to update ESTIMATED_RETURNS_TBL.loc[s,a] much like we do in Deep Learning.\n",
    "\n",
    "Todo:\n",
    "- Complete the missing code in the next cell.\n",
    "- Run the next few cells to get a policy and evaluate it.\n",
    "- Does the policy help the agent attain its goal?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "ESTIMATED_RETURNS_TBL = pd.DataFrame.from_dict({s:{a:0 for a in foolsball.actions} for s in range(foolsball.n_states)}, orient='index')\n",
    "\n",
    "n_episodes = 5000\n",
    "epsilon = 1\n",
    "min_epsilon = 0.1\n",
    "epsilon_decay = 0.999\n",
    "\n",
    "alpha = 0.001\n",
    "\n",
    "for i in range(n_episodes):\n",
    "    estimated_returns = ESTIMATED_RETURNS_TBL\n",
    "  \n",
    "    epsilon = max(epsilon,min_epsilon)\n",
    "    episode_i = collect_epsilon_greedy_episode_from_returns_tbl(estimated_returns,epsilon=epsilon)\n",
    "    epsilon *= epsilon_decay\n",
    "    states, actions, discounted_returns = discounted_return_from_episode(episode_i, gamma=HYPER_PARAMS['gamma'])\n",
    "\n",
    "    for s,a,ret in zip(states, actions, discounted_returns):\n",
    "        ESTIMATED_RETURNS_TBL.loc[s,a] += alpha*(ret - ESTIMATED_RETURNS_TBL.loc[s,a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           n         e         w         s\n",
      "0  -1.335669 -1.274971 -1.322460  1.561024\n",
      "1  -0.817273 -0.991860 -0.425270 -0.805761\n",
      "2   0.000000  0.000000  0.000000  0.000000\n",
      "3   0.000000  0.000000  0.000000  0.000000\n",
      "4  -0.661067  1.781558 -0.883861 -0.673061\n",
      "5  -0.423559  2.186591 -0.283364 -1.369836\n",
      "6  -0.815661 -0.943448  0.009848  2.832764\n",
      "7   0.000000  0.000000  0.000000  0.000000\n",
      "8  -0.444551 -0.556782 -0.470165 -0.132695\n",
      "9   0.000000  0.000000  0.000000  0.000000\n",
      "10  0.164513  0.106824 -0.718256  3.435748\n",
      "11 -0.054726 -0.042920  0.254875 -0.084323\n",
      "12 -0.188650  0.098962 -0.244361 -0.221987\n",
      "13 -0.128388  0.696479 -0.040967 -0.205752\n",
      "14  0.205418 -0.605414  0.268895  4.092016\n",
      "15  0.000000  0.000000  0.000000  0.000000\n",
      "16 -0.093681 -0.113744 -0.113404 -0.178579\n",
      "17  0.000000  0.000000  0.000000  0.000000\n",
      "18  0.398588  4.819536 -0.552334  0.320179\n",
      "19  0.000000  0.000000  0.000000  0.000000\n",
      "{0: 's', 1: 'w', 2: None, 3: 'n', 4: 'e', 5: 'e', 6: 's', 7: None, 8: 's', 9: None, 10: 's', 11: 'w', 12: 'e', 13: 'e', 14: 's', 15: None, 16: 'n', 17: None, 18: 'e', 19: None}\n",
      " 🡓  🡐  ⬤  🡑 \n",
      " 🡒  🡒  🡓  ⬤ \n",
      " 🡓  ⬤  🡓  🡐 \n",
      " 🡒  🡒  🡓  ⬤ \n",
      " 🡑  ⬤  🡒  ⬤ \n"
     ]
    }
   ],
   "source": [
    "estimated_returns = ESTIMATED_RETURNS_TBL\n",
    "print(estimated_returns)\n",
    "\n",
    "policy5 = greedy_policy_from_returns_tbl(estimated_returns)\n",
    "print(policy5)\n",
    "\n",
    "pretty_print_policy(policy5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How can we get faster convergence?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Try the SARSA and Q-learning appraches described [here](https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#sarsa-on-policy-td-control) "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Rediscovering_RL_Notebook_0_SOLVED.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

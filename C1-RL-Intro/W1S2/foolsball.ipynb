{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Rediscovering_RL_Notebook_0_SOLVED.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/farhanhubble/discover-drl/blob/master/Rediscovering_RL_Notebook_0_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DU18ufm7vQYs",
        "colab_type": "text"
      },
      "source": [
        "## Author: Farhan Ahmad <farhanhubble@gmail.com>\n",
        "## ðŸ¦ : [@farhanhubble](https://twitter.com/farhanhubble)\n",
        "## LinkedIN: https://www.linkedin.com/in/farhanhubble/\n",
        "## Bug reports: farhanhubble@gmail.com\n",
        "--- "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cv9hgPks9LhM"
      },
      "source": [
        "# Reinforcement learning with Foolsball\n",
        "- Reinforcement learning is learning to make decisions from experience.\n",
        "- Games are a good testbed for agents to interact with an environment and explore it.\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NWEnCb0m9ve8"
      },
      "source": [
        "# About Foolsball\n",
        "- 5x4 playground that provides a football/foosball-like environment.\n",
        "- An agent or actor:\n",
        "  - always spawned in the top-left corner\n",
        "  - displayed as 'âš½'\n",
        "  - can move North, South, East or West.\n",
        "  - can be controlled algorithmically\n",
        "- A number of **static** opponents, each represented by ðŸ‘•, that occupy certain locations on the field.\n",
        "- A goalpost ðŸ¥… that is fixed in the bottom right corner\n",
        "\n",
        "## Primary goal\n",
        "- We want the agent to learn to reach the goalpost \n",
        "\n",
        "## Secondary goals\n",
        "- We may want the agent to learn to be efficient in some sense, for example, take the shortest path to the goalpost. **More precisely we want an algorithm to learn to control the agent and steer it towards the goalpost.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KcMMYrax7MVv",
        "colab": {}
      },
      "source": [
        "agent = 'âš½'\n",
        "opponent = 'ðŸ‘•'\n",
        "goal = 'ðŸ¥…'\n",
        "\n",
        "arena = [['âš½', ' ' , 'ðŸ‘•', ' ' ],\n",
        "         [' ' , ' ' , ' ' , 'ðŸ‘•'],\n",
        "         [' ' , 'ðŸ‘•', ' ' , ' ' ],\n",
        "         [' ' , ' ' , ' ' , 'ðŸ‘•'],\n",
        "         [' ' , 'ðŸ‘•', ' ' , 'ðŸ¥…']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GKaYR3mmAc6o"
      },
      "source": [
        "# Implementing an environment for the game of Foolsball\n",
        "- OpenAI Gym has many [text environments](https://github.com/openai/gym/tree/master/gym/envs/toy_text).\n",
        "- Text environments are simple to render in a notebook and super-fast to experiment with.\n",
        "- We want to build out own environment for two reasons:\n",
        "  - It's a great exercise in understanding the finer details, like states, actions, rewards, returns.\n",
        "  - Some of the experimentation we do requires looking under the hood of the environment, which is easier with your own implementation than OpenAI Gym.\n",
        "  - OpenAI Gym has a simple `step(), reset()` API that we also implement. So porting our implementation over to Gym should be easy (and fun)!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fyjTlB4cCysH"
      },
      "source": [
        "# Understanding the first bits of terminology.\n",
        "## State \n",
        "- In RL, state refers to information about the environemnt and then agent.\n",
        "- An RL algorithm inspects the state to decide which action to take.\n",
        "- Exactly what information gets captured in `state` depends on a few factors:\n",
        "  - The complexity of the environment: \n",
        "    - The number of actors, \n",
        "    - the nature of the environment, for example text or images. \n",
        "  - The complexity of the algorithm\n",
        "    - A simple algorithm may only need information about the agent and its immediate surroundings.\n",
        "    - A more complex algorithm may need information about the whole environment.\n",
        "\n",
        "\n",
        "## Setup\n",
        "- In our case we want the algorithm to only know about the location of the agent on the field. \n",
        "- We could have included information about the opponents too which would perhaps aid in the decision making but we chose not to.  \n",
        "\n",
        "- The state therefore is a tuple: (row, col), representing the location of the agent. \n",
        "- There are 20 possible values that `state` can take on:\n",
        "  - `row` can range from 0 through 4\n",
        "  - `col` can range from 0 through 3\n",
        "\n",
        "## Implementation details\n",
        "- The state is actually stored as a single integer that can take on values between 0 and 19."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5auCn-_GLVeU"
      },
      "source": [
        "## Actions\n",
        "The agents can perfrom actions in an environment.\n",
        "\n",
        "## Setup\n",
        "- Our agent can perform one kind of action: navigate up, down, right or left.\n",
        "- It has 4 actions: 'n', 'e', 'w', 's'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qjxD0m0uMfbU"
      },
      "source": [
        "# Learning from experience\n",
        "Any RL set up can be modeled as shown below:\n",
        "\n",
        "![](https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcTMDmrmnl_dAyjCOErHPak2gLXmQTgQnVT8gQ&usqp=CAU)\n",
        "\n",
        "- The agent performs an action in the environment\n",
        "- The state of the environment and agent change as a result\n",
        "- The agent receives a reward and the updated state from the environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ASC5S75zN8mO"
      },
      "source": [
        "## Rewards\n",
        "- Reward is the signal that an agent receives after it performs an action.\n",
        "- The reward structure has to be decided by us. \n",
        "- The biggest challenge of RL is that reward is often sparse. \n",
        "\n",
        "## Set up\n",
        "- In our case the reward depends on the rules of the game and our goal.\n",
        "  - If the agent runs into an opponent, the game gets over and the reward is negative (penalizes the agent).\n",
        "  - If the agent makes it to the goalpost, the game gets over and the reward is positive.\n",
        "  - if the agent takes the ball out of the field the reward is negative.\n",
        "  - If the agent makes a valid move what shoud the reward be?\n",
        "\n",
        "## Implementation\n",
        "- The default reward structure in our case is  `{'unmarked':-1, 'opponent':-5, 'outside':-1, 'goal':+5}`\n",
        "- This can be changed at any time by calling `set_rewards()`.\n",
        "- Taking the ball to an unmarked position seems like a small step towards reaching the goalpost. Why would we then ever want to have a negative reward for this type of manouver?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nYOQ27nFWI9C"
      },
      "source": [
        "# Let's start!\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "R2--uKoiUHts"
      },
      "source": [
        "# Step 1: Build the Foolsball environment\n",
        "The code below provides a skeleton for the **Foolsball** environment we want our agent to train in. Fill in the code marked with #Todo to create a working environment.\n",
        "\n",
        "1. Go to the `__init__()` method and try to understand what it is doing\n",
        "  1. Look at the `deserialize()` method and complete all todos.\n",
        "      1. Complete the `__to_state_()` and `__to_indices__()` methods.\n",
        "2. Complete the `reset()` method.\n",
        "3. Go to the `step()` method and understand its intended behavior.\n",
        "  1. Complete `__get_next_state_on_action__()`\n",
        "  2. Complete `__get_reward_for_transition__()`\n",
        "  3. Complete the `step()` method.\n",
        "\n",
        "\n",
        "4. Read through the `render()` method to understand how we display the environment in the different situations. \n",
        "\n",
        "5. Execute the cell below and make sure there are no errors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "faVO8xdj7ZLA",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class Foolsball(object):\n",
        "\n",
        "  def __to_state__(self,row,col):\n",
        "    \"\"\"Convert from integer state to indices (row,col).\"\"\"\n",
        "    return #Todo\n",
        "\n",
        "  def __to_indices__(self, state):\n",
        "    \"\"\"Convert indices(row,col) to state (single integer).\"\"\"\n",
        "    row = #Todo\n",
        "    col = #Todo\n",
        "    return row,col\n",
        "\n",
        "  def __deserialize__(self,map:list,agent:str,opponent:str, goal:str):\n",
        "    \"\"\"Convrt a string representation of a map into a 2D numpy array\n",
        "    Param map: list of lists of strings representing the player, opponents and goal.\n",
        "    Param agent: string representing the agent on the map \n",
        "    Param opponent: string representing every instance of an opponent player\n",
        "    Param goal: string representing the location of the goal on the map\n",
        "    \"\"\"\n",
        "    ## Capture dimensions and map.\n",
        "    self.n_rows = #Todo\n",
        "    self.n_cols = #Todo\n",
        "    self.n_states = #Todo\n",
        "    self.map = #Todo\n",
        "\n",
        "    ## Store string representations for printing the map, etc.\n",
        "    self.agent_repr = #Todo\n",
        "    self.opponent_repr  = #Todo\n",
        "    self.goal_repr = #Todo\n",
        "\n",
        "    ## Find initial state, the desired goal state and the state of the opponents. \n",
        "    self.init_state = None\n",
        "    self.goal_state = None\n",
        "    self.opponents_states = []\n",
        "\n",
        "    for row in range(self.n_rows):\n",
        "      for col in range(self.n_cols):\n",
        "\n",
        "        if map[row][col] == agent:\n",
        "          # Store the initial state outside the map.\n",
        "          # This helps in quickly resetting the game to the initial state and\n",
        "          # also simplifies printing the map independent of the agent's state. \n",
        "          self.init_state = #Todo\n",
        "          self.map[row,col] = ' ' \n",
        "        \n",
        "        elif map[row][col] == opponent:\n",
        "          #Todo\n",
        "\n",
        "        elif map[row][col] == goal:\n",
        "          #Todo\n",
        "\n",
        "    assert self.init_state is not None, print(f\"Map {map} does not specify an agent {agent} location\")\n",
        "    assert self.goal_state is not None,  print(f\"Map {map} does not specify a goal {goal} location\")\n",
        "    assert self.opponents_states,  print(f\"Map {map} does not specify any opponents {opponent} location\")\n",
        "\n",
        "    return self.init_state\n",
        "\n",
        "\n",
        "  def __get_next_state_on_action__(self,state,action):\n",
        "    \"\"\"Return next state based on current state and action.\"\"\"\n",
        "    row, col = self.__to_indices__(state)\n",
        "    action_to_index_delta = {'n':[-1,0], 'e':[0,+1], 'w':[0,-1], 's':[+1,0]}\n",
        "\n",
        "    row_delta, col_delta = action_to_index_delta[action]\n",
        "    new_row , new_col = row+row_delta, col+col_delta\n",
        "\n",
        "    ## Return current state if next state is invalid\n",
        "    if #Todo\n",
        "      return state  \n",
        "\n",
        "    ## Construct state from new row and col and return it.    \n",
        "    return #Todo\n",
        "\n",
        "\n",
        "  def __get_reward_for_transition__(self,state,next_state):\n",
        "    \"\"\" Return the reward based on the transition from current state to next state. \"\"\"\n",
        "    ## Transition rejected due to illegal action (move)\n",
        "    if next_state == state:\n",
        "      reward = #Todo\n",
        "    \n",
        "    ## Goal!\n",
        "    elif next_state == self.goal_state:\n",
        "      reward = #Todo\n",
        "    \n",
        "    ## Ran into opponent. \n",
        "    elif next_state in self.opponents_states:\n",
        "      reward = self.rewards['opponent']\n",
        "\n",
        "    ## Made a safe and valid move.   \n",
        "    else:\n",
        "      reward = #Todo\n",
        "\n",
        "    return reward\n",
        "\n",
        "\n",
        "  def __is_terminal_state__(self, state):\n",
        "    return (state == self.goal_state) or (state in self.opponents_states) \n",
        "\n",
        "  \n",
        "  def __init__(self,map,agent,opponent,goal):\n",
        "    \"\"\"Spawn the world, create variables to track state and actions.\"\"\"\n",
        "    # We just need to track the location of the agent (the ball)\n",
        "    # Everything else is static and so a potential algorithm doesn't \n",
        "    # have to look at it. The variable `done` flags terminal states.\n",
        "    self.state = self.__deserialize__(map,agent,opponent,goal)\n",
        "    self.done = False\n",
        "    self.actions = ['n','e','w','s']\n",
        "\n",
        "    # Set up the rewards\n",
        "    self.default_rewards = {'unmarked':-1, 'opponent':-5, 'outside':-1, 'goal':+5}\n",
        "    self.set_rewards(self.default_rewards)\n",
        "\n",
        "\n",
        "\n",
        "  def reset(self):\n",
        "    \"\"\"Reset the environment to its initial state.\"\"\"\n",
        "    # There's really just two things we need to reset: the state, which should\n",
        "    # be reset to the initial state, and the `done` flag which should be \n",
        "    # cleared to signal that we are not in a terminal state anymore, even if we \n",
        "    # were earlier. \n",
        "    self.state = #Todo\n",
        "    self.done  = #Todo\n",
        "    return self.state\n",
        "\n",
        "  \n",
        "  def set_rewards(self,rewards):\n",
        "    if not self.state == self.init_state:\n",
        "      print('Warning: Setting reward while not in initial state! You may want to call reset() first.')\n",
        "    for key in self.default_rewards:\n",
        "      assert key in rewards, print(f'Key {key} missing from reward.') \n",
        "    self.rewards = rewards\n",
        "\n",
        "  \n",
        "  def step(self,action):\n",
        "    \"\"\"Simulate state transition based on current state and action received.\"\"\"\n",
        "    assert not self.done, \\\n",
        "    print(f'You cannot call step() in a terminal state({self.state}). Check the \"done\" flag before calling step() to avoid this.')\n",
        "    next_state = #Todo: Get next state for this (state,action) pair.\n",
        "\n",
        "    reward = #Todo: Get the reward for the state -> next_state transition.\n",
        "\n",
        "    done = #Todo: Set the flag if we are in a terminal state.\n",
        "\n",
        "    self.state, self.done = next_state, done\n",
        "    \n",
        "    return next_state, reward, done\n",
        "\n",
        "\n",
        "  def render(self):\n",
        "    \"\"\"Pretty-print the environment and agent.\"\"\"\n",
        "    ## Create a copy of the map and change data type to accomodate\n",
        "    ## 3-character strings\n",
        "    _map = np.array(self.map, dtype='<U3')\n",
        "\n",
        "    ## Mark unoccupied positions with special symbol.\n",
        "    ## And add extra spacing to align all columns.\n",
        "    for row in range(_map.shape[0]):\n",
        "      for col in range(_map.shape[1]):\n",
        "        if _map[row,col] == ' ':\n",
        "          _map[row,col] = ' + '\n",
        "        \n",
        "        elif _map[row,col] == self.opponent_repr: \n",
        "          _map[row,col] =  self.opponent_repr + ' '\n",
        "        \n",
        "        elif _map[row,col] == self.goal_repr:\n",
        "          _map[row,col] = ' ' + self.goal_repr + ' '\n",
        "      \n",
        "    ## If current state overlaps with the goal state or one of the opponents'\n",
        "    ## states, susbstitute a distinct marker.\n",
        "    if self.state == self.goal_state:\n",
        "      r,c = self.__to_indices__(self.state)\n",
        "      _map[r,c] = ' ðŸ '\n",
        "    elif self.state in self.opponents_states:\n",
        "      r,c = self.__to_indices__(self.state)\n",
        "      _map[r,c] = ' â— '\n",
        "    else:\n",
        "      r,c = self.__to_indices__(self.state)\n",
        "      _map[r,c] = ' ' + self.agent_repr\n",
        "    \n",
        "    for row in range(_map.shape[0]):\n",
        "      for col in range(_map.shape[1]):\n",
        "        print(f' {_map[row,col]} ',end=\"\")\n",
        "      print('\\n') \n",
        "    \n",
        "    print()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MWXHSX9IWm9o"
      },
      "source": [
        "# Step 2: Verify the environment\n",
        "Execute the two cell below and ensure that there are no runtime error and the rendering happens correctly. You should see output like this:\n",
        "\n",
        "```\n",
        "  âš½   +   ðŸ‘•    +  \n",
        "\n",
        "  +    +    +   ðŸ‘•  \n",
        "\n",
        "  +   ðŸ‘•    +    +  \n",
        "\n",
        "  +    +    +   ðŸ‘•  \n",
        "\n",
        "  +   ðŸ‘•    +    ðŸ¥…  \n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hK5uEU1QGddR",
        "colab": {}
      },
      "source": [
        "foolsball = Foolsball(arena, agent, opponent, goal)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5nn8RNR1NDZK",
        "colab": {}
      },
      "source": [
        "foolsball.render()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xjQffds2YGlj"
      },
      "source": [
        "# Step 3: Explore the environment\n",
        "- Run the next cell to play with the environment and score a few goals. \n",
        "- If there are any errors you may want to go back and update the code for the `Foolsball` class. \n",
        "- Make sure to run the cell containing `foolsball = Foolsball(arena, agent, opponent, goal)` if you update the class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "B4w1HCRHwG60",
        "colab": {}
      },
      "source": [
        "## Move: n,s,e,w\n",
        "## Reset: r\n",
        "## Exit: x\n",
        "while True:\n",
        "  try:\n",
        "    act = input('>>')\n",
        "\n",
        "    if act in foolsball.actions:\n",
        "      print(foolsball.step(act))\n",
        "      print()\n",
        "      foolsball.render()\n",
        "    elif act == 'r':\n",
        "      print(foolsball.reset())\n",
        "      print()\n",
        "      foolsball.render()\n",
        "    elif act == 'x':\n",
        "      break\n",
        "    else:\n",
        "      print(f'Invalid input:{act}')\n",
        "  except Exception as e:\n",
        "    print(e)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "A0Fx2b9DdMxh"
      },
      "source": [
        "# Step 4: Understand the concept of returns\n",
        "- Complete the `get_return()` function.\n",
        "- Calculate returns for a few sample paths by running the next few cells"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tyzJFjD5FRov",
        "colab": {}
      },
      "source": [
        "## Reward and return\n",
        "path1 = ['e','s','e','s','s','s','e']\n",
        "path2 = ['s','e','e','s','s','s','e']\n",
        "path3 = ['s','s','s','e','e','s','e']\n",
        "path4 = ['s','s','s','s','n','e','e','s','e']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fJqLJA8jQCyk",
        "colab": {}
      },
      "source": [
        "def get_return(path):\n",
        "  foolsball.reset()\n",
        "  foolsball.render()\n",
        "  \n",
        "  _return_ = 0\n",
        "  for act in path: \n",
        "    next_state, reward, done = foolsball.step(act)\n",
        "    foolsball.render()\n",
        "    _return_ += reward\n",
        "    \n",
        "    if done:\n",
        "      break\n",
        "    \n",
        "  print(f'Return (accumulated reward): {_return_}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GyqzhRJeR3eL",
        "colab": {}
      },
      "source": [
        "get_return(path1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5kGOePDNR7vz",
        "colab": {}
      },
      "source": [
        "get_return(path2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZyqLBXy0Sncw",
        "colab": {}
      },
      "source": [
        "get_return(path3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jFJS37teSpQe",
        "colab": {}
      },
      "source": [
        "get_return(path4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5bBwFGPFe9Ld"
      },
      "source": [
        "# Step 5: Experiment with a different reward structure.\n",
        "- Does it encourage the agent to take the shortest route?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EdzQu5qmStXR",
        "colab": {}
      },
      "source": [
        "## Different reward structure\n",
        "foolsball.set_rewards({'unmarked':0, 'opponent':-5, 'outside':-1, 'goal':+5})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MXTnL0IPVO1D",
        "colab": {}
      },
      "source": [
        "get_return(path1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2RdLqt9FVcCp",
        "colab": {}
      },
      "source": [
        "get_return(path4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gEQw1TCBfVXa"
      },
      "source": [
        "# Step 6: Learn about discounted returns\n",
        "- Get introduced to discounted return as a means to set acceptable time horizons.\n",
        "$$Discounted\\ Return = R_{t_1} + \\gamma*R_{t_2} + \\gamma^2*R_{t_3} + ... + \\gamma^{n-1}*R_{t_n}$$\n",
        "where $R_{t_k}$  is the reward after step `k` and $\\gamma$ is called the discount factor. \n",
        "- Complete the code below to implement discounted returns.\n",
        "- The discount factor $\\gamma$ is a hyperparameter (why?) often set to 0.9 \n",
        "ðŸ˜œ\n",
        "- Run the next few cells to see if discounting indeed has the effect we want (shorter paths have higher rewards)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "csaEvig4WuCO",
        "colab": {}
      },
      "source": [
        "def get_discounted_return(path, gamma=0):\n",
        "  foolsball.reset()\n",
        "  foolsball.render()\n",
        "  _return_ = 0\n",
        "  discount_coeff = 1\n",
        "  for act in path: \n",
        "    #Todo: Run one step and get the state, action, reward tuple\n",
        "    #Todo: Update discounted return \n",
        "    #Todo: Update the discount multiplier to pow(gamma,n-1)\n",
        "    \n",
        "    foolsball.render()\n",
        "    if done:\n",
        "      break\n",
        "    \n",
        "  print(f'Return (accumulated reward): {_return_}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nBuFLEMLEUbr",
        "colab": {}
      },
      "source": [
        "HYPER_PARAMS = {'gamma':0.9}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "StLUFUgjW_cz",
        "colab": {}
      },
      "source": [
        "get_discounted_return(path1, HYPER_PARAMS['gamma'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GtaApbodXVgp",
        "colab": {}
      },
      "source": [
        "get_discounted_return(path4, HYPER_PARAMS['gamma'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cibo9KfQiBxm"
      },
      "source": [
        "# Step 7: Formalizing the problem:\n",
        "- We want the agent to reach the goalpost AND attain the highest **discounted return**.\n",
        "- This means making safe and efficient moves\n",
        "  - Running into opponents means game over\n",
        "  - Repeated 'outsides' means inefficiency\n",
        "  - Long detours are also inefficient\n",
        "\n",
        "## The Conundrum\n",
        "- We already know how to compute the discounted return for a single path.\n",
        "- We can generate all possible paths and calculate their returns and pick a path with the highest return.\n",
        "\n",
        "- Alas there are too many paths (4 possible decisions at each step => combinatorial explosion).\n",
        "\n",
        "\n",
        "## The \"Trick\"\n",
        "- Even though there are too many paths, all of them are made up of a smaller number of (state,action) pairs.\n",
        "- We can calculate the return for each of the 80(=20x4) state action pairs.\n",
        "- To emphasize, we want to calculate the return for each (state,action) pair, not just the reward.\n",
        "  - Calculating return means peeking into the future, beyond that (state,action) pair.\n",
        "\n",
        "\n",
        "## Todo:\n",
        "- As a precursor to calculating returns for every (state,action) pair, let's try to calculate the reward for every (state,action) pair.\n",
        "\n",
        "- Understand how the code in the next two cells creates a Pandas table to store the rewards for every (state, action) pair.\n",
        "\n",
        "- We will cheat a little by using private methods of the `Foolsball` class\n",
        "  - Use the `__get_next_state_on_action__()` and `__get_reward_for_transition__()` methods to complete the code in the third cell below.\n",
        "  - Run the fourth cell to view the rewards table. \n",
        "  - Notice that rewards for terminal states are kept undefined since no actions are allowed in those states.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jNH54KMKpvua",
        "colab": {}
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yascBBM5uLwC",
        "colab": {}
      },
      "source": [
        "REWARDS_TBL = pd.DataFrame.from_dict({s:{a:None for a in foolsball.actions} for s in range(foolsball.n_states)}, orient='index')\n",
        "REWARDS_TBL"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BJRlz7nBuQxd",
        "colab": {}
      },
      "source": [
        "for state in REWARDS_TBL.index:\n",
        "  if not foolsball.__is_terminal_state__(state): #Only calculate rewards for non-terminal states\n",
        "    for action in REWARDS_TBL.columns:\n",
        "      next_state = #Todo: Get the next state for this (state, action) pair\n",
        "      REWARDS_TBL.loc[state, action] = #Todo: Get the reward for state -> next_state transition."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5u-rbqyavKWu",
        "colab": {}
      },
      "source": [
        "terminal_states = foolsball.opponents_states+[foolsball.goal_state]\n",
        "print(terminal_states)\n",
        "REWARDS_TBL"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OtEnjtynnkaI"
      },
      "source": [
        "#Step 8: Learn about returns table\n",
        "Create a returns table (no TODOs here)\n",
        "- Run the next four cells and guess why we are setting the returns for terminal states to 0.\n",
        "  - We leave the returns for all non-terminal states undefined.\n",
        "  - Trying to fill up these entries will be the focus of the rest of the notebook.\n",
        "\n",
        "- A function to create new instances of the returns table is also provided in the fourth cell below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BiWIpBIj0sql",
        "colab": {}
      },
      "source": [
        "RETURNS_TBL = pd.DataFrame.from_dict({s:{a:None for a in foolsball.actions} for s in range(foolsball.n_states)}, orient='index')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cGFzW7Xb886q",
        "colab": {}
      },
      "source": [
        "RETURNS_TBL"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IKCqzD9_8-E7",
        "colab": {}
      },
      "source": [
        "RETURNS_TBL.loc[terminal_states]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gVay2Rer9eQI",
        "colab": {}
      },
      "source": [
        "RETURNS_TBL.loc[terminal_states] = 0\n",
        "RETURNS_TBL"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ft8yVJdJWd_W",
        "colab": {}
      },
      "source": [
        "def make_returns_table(terminal_states):\n",
        "  \"\"\"Create an empty returns table.\"\"\"\n",
        "  table = pd.DataFrame.from_dict({s:{a:None for a in foolsball.actions} for s in range(foolsball.n_states)}, orient='index')\n",
        "  table.loc[terminal_states] = 0\n",
        "  return table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lXXnzbJEpDuY"
      },
      "source": [
        "# Step 9: Use dynamic programming to fill up the returns table.\n",
        "- The return for a (state, action) can be defined in terms of returns of the next state. \n",
        "  - $Return(state_t,action_t) = Reward(state_t,state_{t+1}) + \\gamma * \\max[Return(state_{t+1}, action=='n'),\\\\ Return(state_{t+1}, action=='e'), \\\\ Return(state_{t+1}, action=='w'), \\\\ Return(state_{t+1}, action=='s')]$\n",
        "  - Can you see why this ought to be?\n",
        "\n",
        "  - This motivates the use of dynamic programming to fill up the returns table.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KLFtYd-AtsgI"
      },
      "source": [
        "## Todo:\n",
        "- Read the code in the next cell and try to understand the dynamic programming based solution. \n",
        "- Run the code in the next cell. The code causes a stack overflow. Why?\n",
        "- Pass debug= True to see what the problem is."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CZKZKPpYM8FD",
        "colab": {}
      },
      "source": [
        "def fill_returns_table_v0(table,state,debug=False): \n",
        "  \"\"\" Recursively fill a returns table, one state at a time.\"\"\"\n",
        "  for action in table.columns:\n",
        "    if table.loc[state][action] is None:\n",
        "      next_state = foolsball.__get_next_state_on_action__(state, action)\n",
        "      reward = foolsball.__get_reward_for_transition__(state, next_state)\n",
        "\n",
        "      if debug:\n",
        "        print(f'Trying to fill ({state},{action},{next_state})')\n",
        "      \n",
        "      fill_returns_table_v0(table, next_state, debug) # <= Earth shaking problem here!!! ðŸ˜±ðŸ˜±ðŸ˜±\n",
        "      table.loc[state][action]  = reward + HYPER_PARAMS['gamma'] * table.loc[next_state].max()\n",
        "    \n",
        "    else:\n",
        "      if debug:\n",
        "        print((state,action),f'already has a RETURN {table.loc[state][action]}')\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xrWAG02iNMkJ",
        "colab": {}
      },
      "source": [
        "table = make_returns_table(terminal_states)\n",
        "fill_returns_table_v0(table,state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Y_O2GDu7uTAw"
      },
      "source": [
        "## Contd..\n",
        "- The code above crashed becasue of indefinite recursion caused by a state,action pairs that resulted in the next state being the same as the current state\n",
        "- We can fix this by catching this case and a returning a large negative return.\n",
        "- Why is the large negative return necessary?\n",
        "- Look at the code below that tries to mitigate this problem with the following code:\n",
        "```\n",
        "      if next_state == state:\n",
        "        table.loc[state][action] = -np.inf # <= No self recursion\n",
        "```\n",
        "\n",
        "- Run the next two cells to see if how the RETURNS table gets filled up."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mFbLxTZ0Gy64",
        "colab": {}
      },
      "source": [
        "def fill_returns_table_v1(table,state,debug=False):\n",
        "  for action in table.columns:\n",
        "    if table.loc[state][action] is None:\n",
        "      next_state = foolsball.__get_next_state_on_action__(state, action)\n",
        "      reward = foolsball.__get_reward_for_transition__(state, next_state)\n",
        "      \n",
        "      if debug:\n",
        "        print(f'Trying to fill ({state},{action},{next_state})')\n",
        "      \n",
        "      if next_state == state:\n",
        "        table.loc[state][action] = -np.inf # <= No self recursion\n",
        "      else:\n",
        "        fill_returns_table_v1(table,next_state,debug)\n",
        "        table.loc[state][action]  = reward + HYPER_PARAMS['gamma'] * table.loc[next_state].max()\n",
        "    else:\n",
        "      if debug:\n",
        "        print((state,action),f'already has a RETURN {table.loc[state][action]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "U1AXgzoQInII",
        "colab": {}
      },
      "source": [
        "table = make_returns_table(terminal_states)\n",
        "fill_returns_table_v1(table, state=0, debug=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hLfbLuhtvbKe"
      },
      "source": [
        "## Contd..\n",
        "- The code above crashed becasue of indefinite mutual recursion caused by a (state,action) pair that resulted in the (next_state, next_action) bringing us back to the first state.\n",
        "- We can fix this by deferring these cases.\n",
        "- Let' see if we can get somewhere.\n",
        "- Run the next few cells to find out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EbpZkdIwJTq1",
        "colab": {}
      },
      "source": [
        "def fill_returns_table_v2(table,state, debug=False):\n",
        "  for action in table.columns:\n",
        "    if table.loc[state][action] is None:\n",
        "      next_state = foolsball.__get_next_state_on_action__(state, action)\n",
        "      reward = foolsball.__get_reward_for_transition__(state, next_state)\n",
        "      \n",
        "      if debug:\n",
        "        print(f'Trying to fill ({state},{action},{next_state})')\n",
        "      \n",
        "      if next_state == state:\n",
        "        table.loc[state][action] = -np.inf # <= No self recursion\n",
        "      \n",
        "      elif not table.loc[next_state].isna().any(): # <= No recursion beyond immediate neighbor!\n",
        "        fill_returns_table_v2(table, next_state)\n",
        "        table.loc[state][action]  = reward + HYPER_PARAMS['gamma'] * table.loc[next_state].max()\n",
        "    \n",
        "    else:\n",
        "      if debug:\n",
        "        print((state,action),f'already has a RETURN {table.loc[state][action]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Mm49G7-LVq83",
        "colab": {}
      },
      "source": [
        "table = make_returns_table(terminal_states)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "v1dniXqKZRIq",
        "colab": {}
      },
      "source": [
        "fill_returns_table_v2(table,state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YDThbMqxZYrl",
        "colab": {}
      },
      "source": [
        "table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bsYhHSYHeC7X"
      },
      "source": [
        "## Contd...\n",
        "- We were able to fill up `(0,'n')` and `(0,'w')`.\n",
        "- Let's try to fill up some of the cells for state `1`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "p4J8aE8qZaPR",
        "colab": {}
      },
      "source": [
        "fill_returns_table_v2(table,state=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vcpD53bxi4js",
        "colab": {}
      },
      "source": [
        "table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cV9rpLNRespb"
      },
      "source": [
        "## Contd...\n",
        "- Non-recursive state,action pairs are fille up for states `0` and `1`.\n",
        "- We will skip over state `2` as it is a terminal state.\n",
        "- Let's try to fill up the table for state `3`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wvzzuhegi5aT",
        "colab": {}
      },
      "source": [
        "fill_returns_table_v2(table,state=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4HkIB9Pzj7Ra",
        "colab": {}
      },
      "source": [
        "table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ikF8qSjgfHbU"
      },
      "source": [
        "## Contd...\n",
        "- Let's try to fill in the rest of the states too. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZBT7-eE-j8ja",
        "colab": {}
      },
      "source": [
        "for s in range(4,19):\n",
        "  fill_returns_table_v2(table,state=s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HfjVh9OAkOrt",
        "colab": {}
      },
      "source": [
        "table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ierdKTiOklYH",
        "colab": {}
      },
      "source": [
        "# Count number of Nones\n",
        "table.isna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UDTSNQB9fbH4"
      },
      "source": [
        "## Contd...\n",
        "- Let's make another pass over all the states to see if some of the cells deferred earlier can be filled now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LwRKvA9rkRBe",
        "colab": {}
      },
      "source": [
        "for s in range(0,19):\n",
        "  fill_returns_table_v2(table,state=s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "spKnLucvkhOi",
        "colab": {}
      },
      "source": [
        "table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kaO1U52hkiUb",
        "colab": {}
      },
      "source": [
        "table.isna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AoxZhHNefr0D"
      },
      "source": [
        "## Contd...\n",
        "- The second pass does not seem to have made any updates.\n",
        "- Let's confirm this by making a third pass over all the states."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mCi6KuKAkpvP",
        "colab": {}
      },
      "source": [
        "for s in range(0,19):\n",
        "  fill_returns_table_v2(table,state=s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AZx5QB-slAHK",
        "colab": {}
      },
      "source": [
        "table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gB4i_CZLlBci",
        "colab": {}
      },
      "source": [
        "table.isna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vFqbsblEgBCN"
      },
      "source": [
        "## Contd...\n",
        "- Unfortunately, it seems that cells will mutual recursion like `(0,'e')` <=> `(1,'w')` never get filled by this algorithm. \n",
        "- The rest of the notebook will try to come up with a non-mutually-recursive solution to the problem of filling up the RETURNS table."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vSucaBX6wH7V"
      },
      "source": [
        "# Get Some more Coffee\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ad0G1-3_-pw0"
      },
      "source": [
        "# Step 10: Estimating returns through simulation\n",
        "- and Monte Carlo sampling\n",
        "- No more dynamic programming \n",
        "- No more cheating by peeping into the environment (private APIs)\n",
        "- Sampling requires simulating the game and trying random actions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EkfoziC6wndC"
      },
      "source": [
        "## Todo\n",
        "- Run the code in the next two cells to collect and print a random episodes.\n",
        "  - The episode starts with the environment in the initial state \n",
        "  - The agent tries random actions\n",
        "  - The episode terminates when the agent collides with an opponent or reaches the goalpost.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ij_84xHhlFZj",
        "colab": {}
      },
      "source": [
        "def collect_random_episode():\n",
        "  state = foolsball.reset()\n",
        "  done = False\n",
        "  episode = []\n",
        "\n",
        "  while not done:\n",
        "    action = np.random.choice(foolsball.actions)\n",
        "    next_state, reward, done = foolsball.step(action)\n",
        "    episode.append([state, action, reward])\n",
        "    state = next_state\n",
        "  \n",
        "  return episode"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TK5xQ6iwvMU_",
        "colab": {}
      },
      "source": [
        "ep = collect_random_episode()\n",
        "foolsball.render()\n",
        "print(ep)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sLbl3Dxmx122"
      },
      "source": [
        "# Step 11: Implement discounted returns for episodes\n",
        "- Complete the function `discounted_return_from_episode()` that computes the discounted return for every (state,action) pair in an episode.\n",
        "  - If an episode is:  $(s_1,a_1,r_1), (s_2,a_2,r_2), (s_3, a_3, r_3)$, **excluding the terminal state**:\n",
        "  - The (discounted) return for $s_1$ is $r_1 + \\gamma * r_2 + \\gamma^2 * r_3$\n",
        "  - The (discounted) return for $s_2$ is $r_2 + \\gamma * r_3$\n",
        "  - The (discounted) return for $s_3$ is $r_3$ \n",
        "\n",
        "- Run the next couple of cells to print discounted returns for entire episodes.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "T_r_tTfSwEIi",
        "colab": {}
      },
      "source": [
        "def discounted_return_from_episode(ep, gamma=0):\n",
        "  states, actions, rewards = list(zip(*ep))\n",
        "  rewards = np.asarray(rewards)\n",
        "  discount_coeffs = np.asarray([np.power(gamma,p) for p in range(len(rewards))])\n",
        "  \n",
        "  l = len(rewards)\n",
        "  discounted_returns = [np.dot(rewards[\"\"\"#Todo: Fill appropriate range\"\"\"],discount_coeffs[\"\"\"#Todo: Fill approriate range.\"\"\"]) for i in range(l)]\n",
        "\n",
        "  return (states, actions, discounted_returns)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sp9dvrQjl_vA",
        "colab": {}
      },
      "source": [
        "discounted_return_from_episode(ep, gamma=HYPER_PARAMS['gamma'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EvgikRgT0FQ-"
      },
      "source": [
        "# Step 12: Estimate returns by simulating lots of episodes.\n",
        "- The code below creates two tables:\n",
        "  - ESTIMATED_RETURNS_TBL for accumulating the return for every (state,action) pair across all episodes.\n",
        "  - VISITS_COUNTS_TBL for storing the number of times a (state,action) pair appears across all episodes.\n",
        "\n",
        "- It then runs an algorithm to generate episodes and fill in these tables.\n",
        "\n",
        "Here's the idea:\n",
        "- Create many random episodes\n",
        "  - Examine each (state, action) pair in every episode.\n",
        "  - Calculate and accumulate the return for this pair\n",
        "    - Since we have the full episode, we can \"see the future\" and calculate the return.\n",
        "    - The return for a (state,action) pair is just a (very bad) estimate of the \"real\" return, since we are looking at just one of the many paths that could possible contain the (state,action).\n",
        "  - Record the visit count of the (state, action) pair.   \n",
        "\n",
        "- At the end the we divide the accumulated returns by the visit counts to get an averaged estimate of the retruns. \n",
        "\n",
        "\n",
        "## Todo:\n",
        "- Complete the code in the **next two cells** to implement what's known as Monte Carlo estimation.\n",
        "- Run the cells to see how well the alorithm fares.\n",
        "- Does the algorithm generate sensible looking returns (estimates)?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "B1R--MLHmalx",
        "colab": {}
      },
      "source": [
        "# Create empty returns table \n",
        "ESTIMATED_RETURNS_TBL = pd.DataFrame.from_dict({s:{a:0 for a in foolsball.actions} for s in range(foolsball.n_states)}, orient='index')\n",
        "VISITS_COUNTS_TBL = pd.DataFrame.from_dict({s:{a:0 for a in foolsball.actions} for s in range(foolsball.n_states)}, orient='index')\n",
        "\n",
        "n_episodes = 100  #Try 100, 500, 2000\n",
        "\n",
        "for i in range(n_episodes):\n",
        "  episode_i = #Todo: Gennerate a random episode\n",
        "  states, actions, discounted_returns = #Todo: Calculate dicounted returns for the episode.\n",
        "\n",
        "  for s,a,ret in zip(states, actions, discounted_returns):\n",
        "    ESTIMATED_RETURNS_TBL.loc[s,a] += #Todo: accumulate the return for this (state,action) pair\n",
        "    VISITS_COUNTS_TBL.loc[s,a] += #Todo: Update visit count.\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GunNiOL6pC7L",
        "colab": {}
      },
      "source": [
        "estimated_returns = ESTIMATED_RETURNS_TBL.div(VISITS_COUNTS_TBL+1) ## Averaging returns. Avoid dividing by zeros.\n",
        "estimated_returns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dtFqZ61zsoUb"
      },
      "source": [
        "# Step 13: Intro to Policies\n",
        "- The estimated returns table is hard to evaluate visually.\n",
        "- To use the table to make decisions, we grab the action with the highest return for each of the states in the table.\n",
        "- This is called a **policy**.\n",
        "- This will be a greedy policy since we take the best action at each state.\n",
        "- Run the next two cells to output a greedy policy derived from the returns table filled earlier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DLYn9_dr56pX",
        "colab": {}
      },
      "source": [
        "def greedy_policy_from_returns_tbl(table):\n",
        "  policy = {s:None for s in table.index }\n",
        "\n",
        "  for state in table.index:\n",
        "    if state not in terminal_states:\n",
        "      greedy_action_index = #Todo: Get the index of the action with the highest return.\n",
        "      greedy_action = table.columns[greedy_action_index]\n",
        "      policy[state] = greedy_action\n",
        "\n",
        "  return policy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2zTVhU_d691b",
        "colab": {}
      },
      "source": [
        "policy0 = greedy_policy_from_returns_tbl(estimated_returns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1w03qgvN7JF-"
      },
      "source": [
        "# Contd..\n",
        "- Here's a function to superimpose a policy over the environment.\n",
        "- Use the code in the next two cells to eyeball the policy we just generated"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EzumT6HE6sq2",
        "colab": {}
      },
      "source": [
        "def pretty_print_policy(policy):\n",
        "  direction_repr = {'n':' ðŸ¡‘ ', 'e':' ðŸ¡’ ', 'w':' ðŸ¡ ', 's':' ðŸ¡“ ', None:' â¬¤ '}\n",
        "\n",
        "  for row in range(foolsball.n_rows):\n",
        "    for col in range(foolsball.n_cols):\n",
        "      state = row * foolsball.n_cols + col\n",
        "      print(direction_repr[policy[state]],end='')\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jBaobgcs647k",
        "colab": {}
      },
      "source": [
        "pretty_print_policy(policy0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Hz2JFP929GfR"
      },
      "source": [
        "# Step 14: Exploiting the information in the returns table.\n",
        "- We are improving our estimates of the returns with each successive episode. \n",
        "- But we are still generating random episodes throughout. \n",
        "- We should also exploit the information we accrue in the RETURNS table.\n",
        "- The implementation below is quite similar to `collect_random_episode()` but here's the key difference:\n",
        "  - In state s, `collect_random_episode()` was returning a random action from ('n', 's', 'e', 'w').\n",
        "  - But from the returns table we know that one of the action, say 'e' generates the best returns so we can make a greedy choice and always return 'e'.\n",
        "  - This is what `collect_greedy_episode_from_returns_tbl()` does.\n",
        "\n",
        "- Run the next to cells to see the difference. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JVIs2nQTj2cD",
        "colab": {}
      },
      "source": [
        "def collect_greedy_episode_from_returns_tbl(table, max_ep_len=20):\n",
        "  state = foolsball.reset()\n",
        "  done = False\n",
        "  episode = []\n",
        "\n",
        "  for _ in range(max_ep_len):\n",
        "    if done:\n",
        "      break\n",
        "    \n",
        "    greedy_action_index = table.loc[state].argmax()\n",
        "    greedy_action = table.columns[greedy_action_index]\n",
        "    next_state, reward, done = foolsball.step(greedy_action)\n",
        "    episode.append([state, greedy_action, reward])\n",
        "    state = next_state\n",
        "  \n",
        "  return episode"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NOxDZGq9yvky",
        "colab": {}
      },
      "source": [
        "collect_greedy_episode_from_returns_tbl(estimated_returns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sHNJRWyWFPR2"
      },
      "source": [
        "# Step 15: Using Greedy Returns\n",
        "## Todo \n",
        "- Implement the loop in the cell below to update the returns table. \n",
        "- The code will be exactly what we used earlier, except that it will use greedy episodes.\n",
        "\n",
        "- Run the next few cells to evaluate the effectiveness.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "E-NSm78dEZXu",
        "colab": {}
      },
      "source": [
        "ESTIMATED_RETURNS_TBL = pd.DataFrame.from_dict({s:{a:0 for a in foolsball.actions} for s in range(foolsball.n_states)}, orient='index')\n",
        "VISITS_COUNTS_TBL = pd.DataFrame.from_dict({s:{a:0 for a in foolsball.actions} for s in range(foolsball.n_states)}, orient='index')\n",
        "\n",
        "n_episodes = 1000\n",
        "\n",
        "for i in range(n_episodes):\n",
        "  #Todo: Implement code block to update ESTIMATED_RETURNS_TBL and VISITS_COUNTS_TBL\n",
        "  # Make sure you are using greedy episodes."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MmDqDS6jEa0M",
        "colab": {}
      },
      "source": [
        "estimated_returns = ESTIMATED_RETURNS_TBL.div(VISITS_COUNTS_TBL+1) ## Averaging returns. Avoid dividing by zeros.\n",
        "estimated_returns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "t_0kWY_vEyDw",
        "colab": {}
      },
      "source": [
        "policy1 = greedy_policy_from_returns_tbl(estimated_returns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AMCzL3AfEl82",
        "colab": {}
      },
      "source": [
        "pretty_print_policy(policy1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "m_idUZnLvtM5"
      },
      "source": [
        "# Contd... \n",
        "Using greedy returns seems to have deteriorated the returns values. Why is that? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZXqSmRG_AVWV"
      },
      "source": [
        "# Step 16: The Exploration-exploitation Dilemma\n",
        "\n",
        "- We have tried pure exploration (with random episodes)\n",
        "- We have also tried pure exploitation (with policy generated from the returns table)\n",
        "- A good agent should try to balance both.\n",
        "\n",
        "\n",
        "## Epsilon-greedy episodes\n",
        "- An epsilon greedy episode blends the previous two approaches\n",
        "- Precisely, when in state `s`:\n",
        "  - The epsilon greedy episode will pick the action yielding the highest returns with a high probability, say 0.8 \n",
        "  - With a low probability it can return one of the suboptimal, random actions.\n",
        "  - The hyperparameter `epsilon` or $\\epsilon$ decides the probability of selecting a random action and `1-epsilon` is the probability of picking the best action. \n",
        "\n",
        "  - Example with epsilon = 0.2 and assuming `w` is the best action\n",
        "    - state `s`\n",
        "    - Actions = ('n','e','w','s')\n",
        "    - Best action (yielding highest return) = 'w'\n",
        "    - Sampling probabilities = $[{\\epsilon \\over 4},{\\epsilon \\over 4}, 1-\\epsilon+{\\epsilon \\over 4},{\\epsilon \\over 4}] = [0.05,0.05,0.85,0.05]$\n",
        "\n",
        "\n",
        "## Todo:\n",
        "Finish the code below and look at how the output differs from the other two methods. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "411GuzIjy0Ai",
        "colab": {}
      },
      "source": [
        "def collect_epsilon_greedy_episode_from_returns_tbl(table, max_ep_len=20, epsilon=0.1):\n",
        "  \n",
        "  state = foolsball.reset()\n",
        "  done = False\n",
        "  episode = []\n",
        "\n",
        "  for _ in range(max_ep_len):\n",
        "    if done:\n",
        "      break\n",
        "    \n",
        "    actions = table.columns\n",
        "    action_probs = np.asarray([epsilon/len(actions)]*len(actions),dtype=np.float)\n",
        "    \n",
        "    greedy_action_index = table.loc[state].argmax()\n",
        "    action_probs[greedy_action_index] += 1-epsilon\n",
        "    \n",
        "    epsilon_greedy_action = #Todo: use np.random.choice() to sample epsilon-greedily\n",
        "\n",
        "    next_state, reward, done = foolsball.step(epsilon_greedy_action)\n",
        "    episode.append([state, epsilon_greedy_action, reward])\n",
        "    state = next_state\n",
        "\n",
        "  return episode"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xoOvdylQnmfF",
        "colab": {}
      },
      "source": [
        "# Generate an epsilon-greedy episode every time. \n",
        "collect_epsilon_greedy_episode_from_returns_tbl(estimated_returns, epsilon=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BzeUyjXYGssT"
      },
      "source": [
        "# Step 17: Epsilon-greedy updates.\n",
        "## Todo:\n",
        "- Run the next few cells to see the effect of using an epsilon greedy approach."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RDMy2xAcBDZE",
        "colab": {}
      },
      "source": [
        "ESTIMATED_RETURNS_TBL = pd.DataFrame.from_dict({s:{a:0 for a in foolsball.actions} for s in range(foolsball.n_states)}, orient='index')\n",
        "VISITS_COUNTS_TBL = pd.DataFrame.from_dict({s:{a:0 for a in foolsball.actions} for s in range(foolsball.n_states)}, orient='index')\n",
        "\n",
        "n_episodes = 1000\n",
        "\n",
        "for i in range(n_episodes):\n",
        "  estimated_returns = ESTIMATED_RETURNS_TBL.div(VISITS_COUNTS_TBL+1)\n",
        "  \n",
        "  episode_i = collect_epsilon_greedy_episode_from_returns_tbl(estimated_returns)\n",
        "  #print(episode_i)\n",
        "  states, actions, discounted_returns = discounted_return_from_episode(episode_i, gamma=HYPER_PARAMS['gamma'])\n",
        "\n",
        "  for s,a,ret in zip(states, actions, discounted_returns):\n",
        "    ESTIMATED_RETURNS_TBL.loc[s,a] += ret\n",
        "    VISITS_COUNTS_TBL.loc[s,a] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xJM0wX3CEzad",
        "colab": {}
      },
      "source": [
        "estimated_returns = ESTIMATED_RETURNS_TBL.div(VISITS_COUNTS_TBL+1) ## Averaging returns. Avoid dividing by zeros.\n",
        "estimated_returns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iHLtgKCyFHsy",
        "colab": {}
      },
      "source": [
        "policy2 = greedy_policy_from_returns_tbl(estimated_returns)\n",
        "policy2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "orYp9J0cHXB9",
        "colab": {}
      },
      "source": [
        "pretty_print_policy(policy2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "J-in0oKYx1IY"
      },
      "source": [
        "# Contd...\n",
        "The policy is better than the greedy policy but does not seem like a clear winner over the purely random policy. What could be going wrong?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6kJ3g6kfHaBN"
      },
      "source": [
        "# Step 18: Revisiting Exploration-Exploitation with Epsilon Decay\n",
        "\n",
        "- What is the best way to balance exploitation with exploration?\n",
        "  - In the beginning, pick absolutely random actions in every state.\n",
        "  - Slowly reduce the randomness to a small value.\n",
        "\n",
        "## Todo:\n",
        "- In the code below pick a value of `epsilon` that makes all actions equiprobable in `collect_epsilon_greedy_episode_from_returns_tbl()`.\n",
        "\n",
        "- Fill in the code to 'anneal' epsilon over episodes. The value of epsilon shoud not drop below the minimum threshold.\n",
        "\n",
        "- Run the next few cells to evaluate this approach.\n",
        "\n",
        "- Does the policy look any better?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k7j3NwXiK7EY",
        "colab": {}
      },
      "source": [
        "ESTIMATED_RETURNS_TBL = pd.DataFrame.from_dict({s:{a:0 for a in foolsball.actions} for s in range(foolsball.n_states)}, orient='index')\n",
        "VISITS_COUNTS_TBL = pd.DataFrame.from_dict({s:{a:0 for a in foolsball.actions} for s in range(foolsball.n_states)}, orient='index')\n",
        "\n",
        "n_episodes = 10000\n",
        "epsilon = #Todo: initialize epsilon to a value that ensures all actions are equiprobable\n",
        "min_epsilon = 0.1\n",
        "epsilon_decay = 0.999\n",
        "\n",
        "for i in range(n_episodes):\n",
        "  estimated_returns = ESTIMATED_RETURNS_TBL.div(VISITS_COUNTS_TBL+1)\n",
        "  \n",
        "  epsilon = #Todo: Pick annealed value unless it is lower than the minimum threshold. \n",
        "  episode_i = collect_epsilon_greedy_episode_from_returns_tbl(estimated_returns,epsilon=epsilon)\n",
        "  epsilon *= epsilon_decay\n",
        "  #print(episode_i)\n",
        "  states, actions, discounted_returns = discounted_return_from_episode(episode_i, gamma=HYPER_PARAMS['gamma'])\n",
        "\n",
        "  for s,a,ret in zip(states, actions, discounted_returns):\n",
        "    ESTIMATED_RETURNS_TBL.loc[s,a] += ret\n",
        "    VISITS_COUNTS_TBL.loc[s,a] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HFezSR8KNmDO",
        "colab": {}
      },
      "source": [
        "estimated_returns = ESTIMATED_RETURNS_TBL.div(VISITS_COUNTS_TBL+1) ## Averaging returns. Avoid dividing by zeros.\n",
        "print(estimated_returns)\n",
        "\n",
        "policy3 = greedy_policy_from_returns_tbl(estimated_returns)\n",
        "print(policy3)\n",
        "\n",
        "pretty_print_policy(policy3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0eIjxpr4sZEu"
      },
      "source": [
        "# Step 19: Constant Alpha\n",
        "\n",
        "## The idea:\n",
        "- Dividing the accumulated returns by visit count has a non linear effect on the updates. (Go back to previous step and see for yourself).\n",
        "\n",
        "- Don't divide at all!\n",
        "\n",
        "- But we need to ensure that updates are small\n",
        "\n",
        "- Idea: \n",
        "\n",
        "  - `ESTIMATED_RETURNS_TBL.loc[s,a]` and `ret` are both estimates of the same quantity. \n",
        "\n",
        "  - Use the difference of the two estimates to update `ESTIMATED_RETURNS_TBL.loc[s,a]` much like we do in Deep Learning.\n",
        "\n",
        "\n",
        "## Todo:\n",
        "- Complete the missing code in the next cell.\n",
        "- Run the next few cells to get a policy and evaluate it.\n",
        "- Does the policy help the agent attain its goal?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "joZ5JyEUrb6J",
        "colab": {}
      },
      "source": [
        "ESTIMATED_RETURNS_TBL = pd.DataFrame.from_dict({s:{a:0 for a in foolsball.actions} for s in range(foolsball.n_states)}, orient='index')\n",
        "\n",
        "n_episodes = 10000\n",
        "epsilon = 1\n",
        "min_epsilon = 0.1\n",
        "epsilon_decay = 0.999\n",
        "\n",
        "alpha = 0.01\n",
        "\n",
        "for i in range(n_episodes):\n",
        "  estimated_returns = ESTIMATED_RETURNS_TBL\n",
        "  \n",
        "  epsilon = max(epsilon,min_epsilon)\n",
        "  episode_i = collect_epsilon_greedy_episode_from_returns_tbl(estimated_returns,epsilon=epsilon)\n",
        "  epsilon *= epsilon_decay\n",
        "  states, actions, discounted_returns = discounted_return_from_episode(episode_i, gamma=HYPER_PARAMS['gamma'])\n",
        "\n",
        "  for s,a,ret in zip(states, actions, discounted_returns):\n",
        "    ESTIMATED_RETURNS_TBL.loc[s,a] += #Todo: Update RHS using hints from the instructions. Use alpha as the learning rate."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YEvhUgzPsmPN",
        "colab": {}
      },
      "source": [
        "estimated_returns = ESTIMATED_RETURNS_TBL\n",
        "print(estimated_returns)\n",
        "\n",
        "policy4 = greedy_policy_from_returns_tbl(estimated_returns)\n",
        "print(policy4)\n",
        "\n",
        "pretty_print_policy(policy4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9qlm5QG3d4fs"
      },
      "source": [
        "# Step 20: What to do next?\n",
        "- Make the world stochastic.\n",
        "  - When in state `s`, action `a` might result in more than one outcomes.\n",
        "  - For example `(0,'e')` might take the agent to state `1` with probability 0.9 and, with probability 0.066, (try to )take the agent north or west, brining it back to state '0', or with proobability 0.033 take the agent south to state `4`. \n",
        "\n",
        "\n",
        "- Make the world dynamic.\n",
        "  - The agent can spawn in any of the unoccupied cells.\n",
        "  - The opponents can move, perhaps chase our agent.\n",
        "    - What extra information might we need to design a good algorithm. \n",
        "\n",
        "- Don't shift the goalpost though. ðŸ˜›\n",
        "\n",
        "- Solve an OpenAI Gym environment, like the [taxi](https://github.com/openai/gym/blob/master/gym/envs/toy_text/taxi.py) problem.\n",
        "\n",
        "- Update the REWARDS table and use the updated table after each step, not after each episode. \n",
        "  - This is known as [SARSA](https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#sarsa-on-policy-td-control) because it uses (S)tate, (A)ction, (R)eward, Next (S)state and Next (A)ction at every step to update the RETURNS table.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zTcpiDWONERY"
      },
      "source": [
        "# Notes:\n",
        "- The Returns table is known as a $Q$ Table in RL literature. \n",
        "- The values in the returns table are called $action\\ values: A$.\n",
        "- The action values corresponding to the highest return are called the $state\\ values: V$ \n",
        "- The policy at any step is often denoted by $\\pi$\n",
        "- The optimal values of $A$, $V$ and $\\pi$ are denoted by $A^*$, $V^*$ and $\\pi^*$\n",
        "- Reinforcement learning algorithms usually start with a randomly initialized $Q$ table/action values $A$, then use it to calculate $V$ and then generate a policy $\\pi$ from $V$. The policy is then used to sample many episodes and get better estimates for $Q$. This can be repeated many many times until we get a good-enough policy.\n",
        "\n",
        "# References\n",
        "- https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html\n",
        "- https://spinningup.openai.com/en/latest/\n",
        "- https://gym.openai.com/\n"
      ]
    }
  ]
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reinforcement Learning in Action\n",
    "## Course1: Introduction to Reinforcement Learning\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Course Syllabus\n",
    "---\n",
    "- What is RL?\n",
    "- What kind of problems does RL solve?\n",
    "- **C1: Foolsball**\n",
    "- Modeling problems using the RL framework: Agent, Environment, State, Goals, Rewards, Returns\n",
    "- MDPs and single-step dynamics\n",
    "- State-values, action values, policies, optimality\n",
    "- Solving MDP with known single-step dynamics\n",
    "- Monte Carlo estimation, greedy policies, exploration and exploitation, epsilon-greedy policies\n",
    "- TD methods: Sarsa, Sarsamax Q-learning, Expected Sarsa\n",
    "- **P1: Taxi-V3**\n",
    "- Discretization: Tile-coding, Kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Week 2 Summary\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Supervised Learning vs Planning and Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "| Supervised Learning                   | Learning Planning/Control                     |\n",
    "|---------------------------------------|--------------------------------------------|\n",
    "|Learning from examples          | Learning from experience |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "| Supervised Learning                   | Learning Planning/Control                     |\n",
    "|---------------------------------------|--------------------------------------------|\n",
    "|Learning from examples          | Learning from experience |\n",
    "|Labels accompany every example | Labels(rewards) are sparse| "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "| Supervised Learning                   | Learning Planning/Control                     |\n",
    "|---------------------------------------|--------------------------------------------|\n",
    "|Learning from examples          | Learning from experience |\n",
    "|Labels accompany every example | Labels(rewards) are sparse| \n",
    "|Trying to maximize the number of correct predictions | Trying to maximize long-term reward|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "| Supervised Learning                   | Learning Planning/Control                     |\n",
    "|---------------------------------------|--------------------------------------------|\n",
    "|Learning from examples          | Learning from experience |\n",
    "|Labels accompany every example | Labels(rewards) are sparse| \n",
    "|Trying to maximize the number of correct predictions | Trying to maximize long-term reward|\n",
    "|Dataset is static, representative and manually labeled | Dataset is dynamic and generated through interaction| \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Supervised Learning Useful for Planning/Control?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Sometimes [\\[1\\]](http://rail.eecs.berkeley.edu/deeprlcoursesp17/docs/week_2_lecture_1_behavior_cloning.pdf) [\\[2\\]](https://images.nvidia.com/content/tegra/automotive/images/2016/solutions/pdf/end-to-end-dl-using-px.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![behavior-cloning.png](res/behavior-cloning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Modeling Learning to Plan and Control From Experience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Environment and Agent Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcTMDmrmnl_dAyjCOErHPak2gLXmQTgQnVT8gQ&usqp=CAU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The agent performs an action in the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The state of the environment and agent change as a result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The agent receives a reward and the updated state from the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The goal is to learn the 'best' action(decision) for every state such that we maximize accumulated reward over a long-term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Converting the problem into Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Construct a returns table (n_states x n_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Fill the table with the highest possible return possible for every (state, action) pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Solving the Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Set the return for all actions in all terminal states to 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Exploit the recursive relationship between $return(state_t, action_t)$ and $return(state_{t+1}, action_{t+1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ Return(state_t,action_t) = Reward(state_t,state_{t+1}) + \\gamma * \\max \\begin{bmatrix} Return(state_{t+1}, action_{t+1}=='n')\\\\ Return(state_{t+1}, action_{t+1}=='e')\\\\  Return(state_{t+1}, action_{t+1}=='w')\\\\  Return(state_{t+1}, action_{t+1}=='s') \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Solving the Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- To avoid running into indefinite, mutual recursion use the an iterative approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "    - Initialize all entries in the returns table to arbitrary values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "    - Update each entry in the returns table using the recursive forumula "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ RETURNS\\_TABLE(state_t,action_t) = Reward(state_t,state_{t+1}) + \\gamma * \\max \\begin{bmatrix} RETURNS\\_TABLE(state_{t+1}, action_{t+1}=='n')\\\\ RETURNS\\_TABLE(state_{t+1}, action_{t+1}=='e')\\\\  RETURNS\\_TABLE(state_{t+1}, action_{t+1}=='w')\\\\  RETURNS\\_TABLE(state_{t+1}, action_{t+1}=='s') \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "    - Repeat the previous step until all values in the table converge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Solution At a Glance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```\n",
    "def make_returns_table(states_list, actions_list, terminal_states):\n",
    "    \"\"\"Create an empty returns table where each entry is initialized arbitrarily.\"\"\"\n",
    "    table = pd.DataFrame.from_dict({s:{a:0 for a in actions_list} for s in states_list}, orient='index')\n",
    "    return table\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```\n",
    "RETURNS_TBL = make_returns_table(range(foolsball.n_states), foolsball.actions, terminal_states)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```\n",
    "def compute_returns(table,state,action, debug=False): \n",
    "    \"\"\" Recursively compute the discounted return for a (state,action) pair\"\"\"\n",
    "    if not foolsball.__is_terminal_state__(state):\n",
    "\n",
    "        next_state = foolsball.__get_next_state_on_action__(state, action)\n",
    "        reward = foolsball.__get_reward_for_transition__(state, next_state)\n",
    "\n",
    "        update = HYPER_PARAMS['gamma'] *\\\n",
    "        max(table.loc[next_state, foolsball.actions[0]],\\\n",
    "        table.loc[next_state, foolsball.actions[1]],\\\n",
    "        table.loc[next_state, foolsball.actions[2]],\\\n",
    "        table.loc[next_state, foolsball.actions[3]])\n",
    "  \n",
    "\n",
    "        table.loc[state, action]  = reward + update\n",
    "        #print(state,action,next_state,reward,update,reward + update)\n",
    "    \n",
    "    return table.loc[state,action]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Solution At a Glance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```\n",
    "for i in range(1,50):\n",
    "    RETURNS_TBL_OLD = RETURNS_TBL.copy()\n",
    "    for s in range(foolsball.n_states):\n",
    "        for a in foolsball.actions:\n",
    "            compute_returns_v2(RETURNS_TBL,state=s, action=a, debug=True)\n",
    "    \n",
    "    if i%5 == 0:\n",
    "        print(f'\\n{i} iterations')\n",
    "        print(RETURNS_TBL)\n",
    "    \n",
    "    deltas = ((RETURNS_TBL- RETURNS_TBL_OLD)).abs().values.max()\n",
    "    if deltas < 1e-3:\n",
    "        print(f'\\nConvergence achieved at {i} iterations')\n",
    "        print(RETURNS_TBL)\n",
    "        break\n",
    "```        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Best Policy from Rewards Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "An optimum policy is a sequence $(state_t, best\\_action_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1: Set state to initial state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2: Pick the action with the highest return in the state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "3: Transition to a new state based on the state and the selected action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "4: Update state to new state and repeat from step 2 until the new state is a goal state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A More Realistic Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Dealing with incomplete Knowledge of the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We may not know all the states of the environment in advance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We may not know the single-step dynamics $(state_{t},action_{t}) \\rightarrow state_{t+1}=??, reward_t=?? $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Interact with the environment by taking random actions a.k.a sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Every step in the interaction provides information about state transitions and accompanying rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The interaction ends once we reach a terminal state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The steps make up an episode "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Calculate return for each step: $(state_t, action_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The return calculated for any $(state_t,action_t)$ pair is not an accurate estimate of the highest possible return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Generate many episodes to get better estimates of the highest possible returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Incremental Improvements (W3S1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Random actions (exploration) do not yield the best returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Instead of taking random actions, pick greedy actions (exploitation) using entries from the returns table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Interleave episode generation and returns table updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Combine random actions strategy and greedy actions strategy (exploration + exploitation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Optimize for faster convergence "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Putting Everything Together Specific Algorithms (W3S2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Markov decision processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- State-values and action-values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Policies: Greedy and Optimal Policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Bellman equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Monte Carlo Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Exploration and exploitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Q learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- SARSA and expected SARSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **P1:OpenAI Gym Taxi V3 Problem**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Week 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Introducing stochasticity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Playing against intellignet adversaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Introducing continuous state/action spaces"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

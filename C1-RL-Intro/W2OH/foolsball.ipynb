{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cv9hgPks9LhM"
   },
   "source": [
    "# Reinforcement learning with Foolsball\n",
    "- Reinforcement learning is learning to make decisions from experience.\n",
    "- Games are a good testbed for RL.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NWEnCb0m9ve8"
   },
   "source": [
    "# About Foolsball\n",
    "- 5x4 playground that provides a football/foosball-like environment.\n",
    "- A controllable player:\n",
    "  - always spawned in the top-left corner\n",
    "  - displayed as '⚽'\n",
    "  - can move North, South, East or West.\n",
    "  - can be controlled algorithmically\n",
    "- A number of **static** opponents, each represented by 👕, that occupy certain locations on the field.\n",
    "- A goalpost 🥅 that is fixed in the bottom right corner\n",
    "\n",
    "## Goals\n",
    "### Primary goal\n",
    "- We want the agent to learn to reach the goalpost \n",
    "\n",
    "### Secondary goals\n",
    "- We may want the agent to learn to be efficient in some sense, for example, take the shortest path to the goalpost. \n",
    "\n",
    "## Rules \n",
    "- Initial rules:\n",
    "    - The ball can be (tried to be) moved in four direction: \\['n','e','w',s'\\]\n",
    "    - Move the ball to an unmarked position: -1 points\n",
    "    - Move the ball to a position marked by a defender: -5 points\n",
    "    - Try to move the ball ouside the field: -1 (ball stays in the previous position)\n",
    "    - Move the ball into the goal post position: +5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "agent = '⚽'\n",
    "opponent = '👕'\n",
    "goal = '🥅'\n",
    "\n",
    "arena = [['⚽', ' ' , '👕', ' ' ],\n",
    "         [' ' , ' ' , ' ' , '👕'],\n",
    "         [' ' , '👕', ' ' , ' ' ],\n",
    "         [' ' , ' ' , ' ' , '👕'],\n",
    "         [' ' , '👕', ' ' , '🥅']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Foolsball(object):\n",
    "    def __to_state__(self,row,col):\n",
    "        \"\"\"Convert from indices (row,col) to integer position.\"\"\"\n",
    "        return row*self.n_cols + col\n",
    "    \n",
    "    \n",
    "    def __to_indices__(self, state):\n",
    "        \"\"\"Convert from inteeger position to indices(row,col)\"\"\"\n",
    "        row = state // self.n_cols\n",
    "        col = state % self.n_cols\n",
    "        return row,col\n",
    "\n",
    "    def __deserialize__(self,map:list,agent:str,opponent:str, goal:str):\n",
    "        \"\"\"Convrt a string representation of a map into a 2D numpy array\n",
    "        Param map: list of lists of strings representing the player, opponents and goal.\n",
    "        Param agent: string representing the agent on the map \n",
    "        Param opponent: string representing every instance of an opponent player\n",
    "        Param goal: string representing the location of the goal on the map\n",
    "        \"\"\"\n",
    "        ## Capture dimensions and map.\n",
    "        self.n_rows = len(map)\n",
    "        self.n_cols = len(map[0])\n",
    "        self.n_states = self.n_rows * self.n_cols\n",
    "        self.map = np.asarray(map)\n",
    "\n",
    "        ## Store string representations for printing the map, etc.\n",
    "        self.agent_repr = agent\n",
    "        self.opponent_repr  = opponent\n",
    "        self.goal_repr = goal\n",
    "\n",
    "        ## Find initial state, the desired goal state and the state of the opponents. \n",
    "        self.init_state = None\n",
    "        self.goal_state = None\n",
    "        self.opponents_states = []\n",
    "\n",
    "        for row in range(self.n_rows):\n",
    "            for col in range(self.n_cols):\n",
    "                if map[row][col] == agent:\n",
    "                    # Store the initial state outside the map.\n",
    "                    # This helps in quickly resetting the game to the initial state and\n",
    "                    # also simplifies printing the map independent of the agent's state. \n",
    "                    self.init_state = self.__to_state__(row,col)\n",
    "                    self.map[row,col] = ' ' \n",
    "\n",
    "                elif map[row][col] == opponent:\n",
    "                    self.opponents_states.append(self.__to_state__(row,col))\n",
    "\n",
    "                elif map[row][col] == goal:\n",
    "                    self.goal_state = self.__to_state__(row,col)\n",
    "\n",
    "        assert self.init_state is not None, f\"Map {map} does not specify an agent {agent} location\"\n",
    "        assert self.goal_state is not None,  f\"Map {map} does not specify a goal {goal} location\"\n",
    "        assert self.opponents_states,  f\"Map {map} does not specify any opponents {opponent} location\"\n",
    "\n",
    "        return self.init_state\n",
    "    \n",
    "    \n",
    "    def __init__(self,map,agent,opponent,goal):\n",
    "        \"\"\"Spawn the world, create variables to track state and actions.\"\"\"\n",
    "        # We just need to track the location of the agent (the ball)\n",
    "        # Everything else is static and so a potential algorithm doesn't \n",
    "        # have to look at it. The variable `done` flags terminal states.\n",
    "        self.state = self.__deserialize__(map,agent,opponent,goal)\n",
    "        self.done = False\n",
    "        self.actions = ['n','e','w','s']\n",
    "\n",
    "        # Set up the rewards\n",
    "        self.default_rewards = {'unmarked':-1, 'opponent':-5, 'outside':-1, 'goal':+5}\n",
    "        self.set_rewards(self.default_rewards)\n",
    "        \n",
    "    def set_rewards(self,rewards):\n",
    "        if not self.state == self.init_state:\n",
    "            print('Warning: Setting reward while not in initial state! You may want to call reset() first.')\n",
    "        for key in self.default_rewards:\n",
    "            assert key in rewards, f'Key {key} missing from reward.'\n",
    "            self.rewards = rewards\n",
    "            \n",
    "            \n",
    "    def reset(self):\n",
    "        \"\"\"Reset the environment to its initial state.\"\"\"\n",
    "        # There's really just two things we need to reset: the state, which should\n",
    "        # be reset to the initial state, and the `done` flag which should be \n",
    "        # cleared to signal that we are not in a terminal state anymore, even if we \n",
    "        # were earlier. \n",
    "        self.state = self.init_state\n",
    "        self.done  = False\n",
    "        return self.state\n",
    "    \n",
    "    def __get_next_state_on_action__(self,state,action):\n",
    "        \"\"\"Return next state based on current state and action.\"\"\"\n",
    "        assert not self.__is_terminal_state__(state), f\"Action {action} undefined for terminal state {state}\"\n",
    "        \n",
    "        row, col = self.__to_indices__(state)\n",
    "        action_to_index_delta = {'n':[-1,0], 'e':[0,+1], 'w':[0,-1], 's':[+1,0]}\n",
    "\n",
    "        row_delta, col_delta = action_to_index_delta[action]\n",
    "        new_row , new_col = row+row_delta, col+col_delta\n",
    "\n",
    "        ## Return current state if next state is invalid\n",
    "        if not(0<=new_row<self.n_rows) or\\\n",
    "        not(0<=new_col<self.n_cols):\n",
    "            return state  \n",
    "\n",
    "        ## Construct state from new row and col and return it.    \n",
    "        return self.__to_state__(new_row, new_col)    \n",
    "    \n",
    "  \n",
    "    def __get_reward_for_transition__(self,state,next_state):\n",
    "        \"\"\" Return the reward based on the transition from current state to next state. \"\"\"\n",
    "        ## Transition rejected due to illegal action (move)\n",
    "        assert not self.__is_terminal_state__(state), f\"Reward is undefined for terminal state {state}\"\n",
    "        \n",
    "        if next_state == state:\n",
    "            reward = self.rewards['outside']\n",
    "\n",
    "        ## Goal!\n",
    "        elif next_state == self.goal_state:\n",
    "            reward = self.rewards['goal']\n",
    "\n",
    "        ## Ran into opponent. \n",
    "        elif next_state in self.opponents_states:\n",
    "            reward = self.rewards['opponent']\n",
    "\n",
    "        ## Made a safe and valid move.   \n",
    "        else:\n",
    "            reward = self.rewards['unmarked']\n",
    "\n",
    "        return reward    \n",
    "    \n",
    "    \n",
    "    def __is_terminal_state__(self, state):\n",
    "        return (state == self.goal_state) or (state in self.opponents_states) \n",
    "    \n",
    "      \n",
    "    def step(self,action):\n",
    "        \"\"\"Simulate state transition based on current state and action received.\"\"\"\n",
    "        assert not self.done, \\\n",
    "        f'You cannot call step() in a terminal state({self.state}). Check the \"done\" flag before calling step() to avoid this.'\n",
    "        next_state = self.__get_next_state_on_action__(self.state, action)\n",
    "\n",
    "        reward = self.__get_reward_for_transition__(self.state, next_state)\n",
    "\n",
    "        done = self.__is_terminal_state__(next_state)\n",
    "\n",
    "        self.state, self.done = next_state, done\n",
    "\n",
    "        return next_state, reward, done\n",
    "    \n",
    "    \n",
    "    \n",
    "    def render(self):\n",
    "        \"\"\"Pretty-print the environment and agent.\"\"\"\n",
    "        ## Create a copy of the map and change data type to accomodate\n",
    "        ## 3-character strings\n",
    "        _map = np.array(self.map, dtype='<U3')\n",
    "\n",
    "        ## Mark unoccupied positions with special symbol.\n",
    "        ## And add extra spacing to align all columns.\n",
    "        for row in range(_map.shape[0]):\n",
    "            for col in range(_map.shape[1]):\n",
    "                if _map[row,col] == ' ':\n",
    "                    _map[row,col] = ' + '\n",
    "\n",
    "                elif _map[row,col] == self.opponent_repr: \n",
    "                    _map[row,col] =  self.opponent_repr + ' '\n",
    "\n",
    "                elif _map[row,col] == self.goal_repr:\n",
    "                    _map[row,col] = ' ' + self.goal_repr + ' '\n",
    "\n",
    "        ## If current state overlaps with the goal state or one of the opponents'\n",
    "        ## states, susbstitute a distinct marker.\n",
    "        if self.state == self.goal_state:\n",
    "            r,c = self.__to_indices__(self.state)\n",
    "            _map[r,c] = ' 🏁 '\n",
    "        elif self.state in self.opponents_states:\n",
    "            r,c = self.__to_indices__(self.state)\n",
    "            _map[r,c] = ' ❗ '\n",
    "        else:\n",
    "            r,c = self.__to_indices__(self.state)\n",
    "            _map[r,c] = ' ' + self.agent_repr\n",
    "\n",
    "        for row in range(_map.shape[0]):\n",
    "            for col in range(_map.shape[1]):\n",
    "                print(f' {_map[row,col]} ',end=\"\")\n",
    "            print('\\n') \n",
    "\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "foolsball = Foolsball(arena, agent, opponent, goal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5nn8RNR1NDZK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ⚽   +   👕    +  \n",
      "\n",
      "  +    +    +   👕  \n",
      "\n",
      "  +   👕    +    +  \n",
      "\n",
      "  +    +    +   👕  \n",
      "\n",
      "  +   👕    +    🥅  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "foolsball.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Override the default reward structure.\n",
    "- Use a more sparse reward: {'unmarked':0, 'opponent':-5, 'outside':-1, 'goal':+5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Update reward structure to: {'unmarked':0, 'opponent':-5, 'outside':-1, 'goal':+5}\n",
    "foolsball.reset()\n",
    "foolsball.set_rewards({'unmarked':0, 'opponent':-5, 'outside':-1, 'goal':+5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement discounted returns¶\n",
    "$$Discounted\\ Return = R_{t_1} + \\gamma*R_{t_2} + \\gamma^2*R_{t_3} + ... + \\gamma^{n-1}*R_{t_n}$$where $R_{t_k}$ is the reward after step k and $\\gamma$ is called the discount factor.\n",
    "- Set the discount factor $\\gamma$ to 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discounted_return(path, gamma=0):\n",
    "    foolsball.reset()\n",
    "    foolsball.render()\n",
    "    _return_ = 0\n",
    "    discount_coeff = 1\n",
    "    for act in path: \n",
    "        next_state, reward, done = foolsball.step(act)\n",
    "        _return_ += discount_coeff*reward\n",
    "        discount_coeff *= gamma    \n",
    "\n",
    "        foolsball.render()\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    print(f'Return (accumulated reward): {_return_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "HYPER_PARAMS = {'gamma':0.9}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use dynamic programming to fill up the returns table\n",
    "- The **highest discounted return** for a **(state, action)** can be defined in terms of returns of the next state.\n",
    "\n",
    "$ Return(state_t,action_t) = Reward(state_t,state_{t+1}) + \\gamma * \\max \\begin{bmatrix} Return(state_{t+1}, action_{t+1}=='n')\\\\ Return(state_{t+1}, action_{t+1}=='e')\\\\  Return(state_{t+1}, action_{t+1}=='w')\\\\  Return(state_{t+1}, action_{t+1}=='s') \\end{bmatrix}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def make_returns_table(states_list, actions_list, terminal_states):\n",
    "    \"\"\"Create an empty returns table where each entry is initialized arbitrarily.\"\"\"\n",
    "    table = pd.DataFrame.from_dict({s:{a:0 for a in actions_list} for s in states_list}, orient='index')\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>e</th>\n",
       "      <th>w</th>\n",
       "      <th>s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    n  e  w  s\n",
       "0   0  0  0  0\n",
       "1   0  0  0  0\n",
       "2   0  0  0  0\n",
       "3   0  0  0  0\n",
       "4   0  0  0  0\n",
       "5   0  0  0  0\n",
       "6   0  0  0  0\n",
       "7   0  0  0  0\n",
       "8   0  0  0  0\n",
       "9   0  0  0  0\n",
       "10  0  0  0  0\n",
       "11  0  0  0  0\n",
       "12  0  0  0  0\n",
       "13  0  0  0  0\n",
       "14  0  0  0  0\n",
       "15  0  0  0  0\n",
       "16  0  0  0  0\n",
       "17  0  0  0  0\n",
       "18  0  0  0  0\n",
       "19  0  0  0  0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terminal_states = foolsball.opponents_states + [foolsball.goal_state]\n",
    "RETURNS_TBL = make_returns_table(range(foolsball.n_states), foolsball.actions, terminal_states)\n",
    "RETURNS_TBL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_returns(table,state,action, debug=False): \n",
    "    \"\"\" Recursively compute the discounted return for a (state,action) pair\"\"\"\n",
    "    if not foolsball.__is_terminal_state__(state):\n",
    "\n",
    "        next_state = foolsball.__get_next_state_on_action__(state, action)\n",
    "        reward = foolsball.__get_reward_for_transition__(state, next_state)\n",
    "\n",
    "        update = HYPER_PARAMS['gamma'] *\\\n",
    "        max(table.loc[next_state, foolsball.actions[0]],\\\n",
    "        table.loc[next_state, foolsball.actions[1]],\\\n",
    "        table.loc[next_state, foolsball.actions[2]],\\\n",
    "        table.loc[next_state, foolsball.actions[3]])\n",
    "\n",
    "        table.loc[state, action]  = reward + update\n",
    "    \n",
    "    return table.loc[state,action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>e</th>\n",
       "      <th>w</th>\n",
       "      <th>s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-5.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-5.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      n    e    w    s\n",
       "0  -1.0  0.0 -1.0  0.0\n",
       "1  -1.0 -5.0  0.0  0.0\n",
       "2   0.0  0.0  0.0  0.0\n",
       "3  -1.0 -1.0 -5.0 -5.0\n",
       "4   0.0  0.0 -1.0  0.0\n",
       "5   0.0  0.0  0.0 -5.0\n",
       "6  -5.0 -5.0  0.0  0.0\n",
       "7   0.0  0.0  0.0  0.0\n",
       "8   0.0 -5.0 -1.0  0.0\n",
       "9   0.0  0.0  0.0  0.0\n",
       "10  0.0  0.0 -5.0  0.0\n",
       "11 -5.0 -1.0  0.0 -5.0\n",
       "12  0.0  0.0 -1.0  0.0\n",
       "13 -5.0  0.0  0.0 -5.0\n",
       "14  0.0 -5.0  0.0  0.0\n",
       "15  0.0  0.0  0.0  0.0\n",
       "16  0.0 -5.0 -1.0 -1.0\n",
       "17  0.0  0.0  0.0  0.0\n",
       "18  0.0  5.0 -5.0  3.5\n",
       "19  0.0  0.0  0.0  0.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for s in range(foolsball.n_states):\n",
    "    for a in foolsball.actions:\n",
    "        compute_returns(RETURNS_TBL,state=s, action=a, debug=True)\n",
    "RETURNS_TBL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let the returns stabilize (converge)¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 iterations\n",
      "          n       e        w        s\n",
      "0  -1.00000  0.0000 -1.00000  0.00000\n",
      "1  -1.00000 -5.0000  0.00000  0.00000\n",
      "2   0.00000  0.0000  0.00000  0.00000\n",
      "3  -4.09510 -4.0951 -5.00000 -5.00000\n",
      "4   0.00000  0.0000 -1.00000  0.00000\n",
      "5   0.00000  3.2805  0.00000 -5.00000\n",
      "6  -5.00000 -5.0000  2.95245  3.64500\n",
      "7   0.00000  0.0000  0.00000  0.00000\n",
      "8   0.00000 -5.0000 -1.00000  3.28050\n",
      "9   0.00000  0.0000  0.00000  0.00000\n",
      "10  3.28050  3.2805 -5.00000  4.05000\n",
      "11 -5.00000  2.2805  3.64500 -5.00000\n",
      "12  2.95245  3.6450  2.28050  2.95245\n",
      "13 -5.00000  4.0500  3.28050 -5.00000\n",
      "14  3.64500 -5.0000  3.64500  4.50000\n",
      "15  0.00000  0.0000  0.00000  0.00000\n",
      "16  3.28050 -5.0000  1.95245  1.95245\n",
      "17  0.00000  0.0000  0.00000  0.00000\n",
      "18  4.05000  5.0000 -5.00000  3.50000\n",
      "19  0.00000  0.0000  0.00000  0.00000\n",
      "\n",
      "Convergence achieved at 9 iterations\n",
      "           n         e         w         s\n",
      "0   1.391485  2.657205  1.391485  2.657205\n",
      "1   1.657205 -5.000000  2.391485  2.952450\n",
      "2   0.000000  0.000000  0.000000  0.000000\n",
      "3  -5.500000 -5.500000 -5.000000 -5.000000\n",
      "4   2.391485  2.952450  1.657205  2.952450\n",
      "5   2.657205  3.280500  2.657205 -5.000000\n",
      "6  -5.000000 -5.000000  2.952450  3.645000\n",
      "7   0.000000  0.000000  0.000000  0.000000\n",
      "8   2.657205 -5.000000  1.952450  3.280500\n",
      "9   0.000000  0.000000  0.000000  0.000000\n",
      "10  3.280500  3.280500 -5.000000  4.050000\n",
      "11 -5.000000  2.280500  3.645000 -5.000000\n",
      "12  2.952450  3.645000  2.280500  2.952450\n",
      "13 -5.000000  4.050000  3.280500 -5.000000\n",
      "14  3.645000 -5.000000  3.645000  4.500000\n",
      "15  0.000000  0.000000  0.000000  0.000000\n",
      "16  3.280500 -5.000000  1.952450  1.952450\n",
      "17  0.000000  0.000000  0.000000  0.000000\n",
      "18  4.050000  5.000000 -5.000000  3.500000\n",
      "19  0.000000  0.000000  0.000000  0.000000\n"
     ]
    }
   ],
   "source": [
    "RETURNS_TBL = make_returns_table(range(foolsball.n_states), foolsball.actions, terminal_states)\n",
    "for i in range(1,50):\n",
    "    RETURNS_TBL_OLD = RETURNS_TBL.copy()\n",
    "    for s in range(foolsball.n_states):\n",
    "        for a in foolsball.actions:\n",
    "            compute_returns(RETURNS_TBL,state=s, action=a, debug=True)\n",
    "    \n",
    "    if i%5 == 0:\n",
    "        print(f'\\n{i} iterations')\n",
    "        print(RETURNS_TBL)\n",
    "    \n",
    "    deltas = RETURNS_TBL- RETURNS_TBL_OLD\n",
    "    if abs(deltas.values.max()) < 1e-3:\n",
    "        print(f'\\nConvergence achieved at {i} iterations')\n",
    "        print(RETURNS_TBL)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_policy_from_returns_tbl(table):\n",
    "    policy = {s:None for s in table.index }\n",
    "    for state in table.index:\n",
    "        if state not in terminal_states:\n",
    "            greedy_action = table.loc[state].idxmax()\n",
    "            policy[state] = greedy_action\n",
    "            \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'e',\n",
       " 1: 's',\n",
       " 2: None,\n",
       " 3: 'w',\n",
       " 4: 'e',\n",
       " 5: 'e',\n",
       " 6: 's',\n",
       " 7: None,\n",
       " 8: 's',\n",
       " 9: None,\n",
       " 10: 's',\n",
       " 11: 'w',\n",
       " 12: 'e',\n",
       " 13: 'e',\n",
       " 14: 's',\n",
       " 15: None,\n",
       " 16: 'n',\n",
       " 17: None,\n",
       " 18: 'e',\n",
       " 19: None}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy0 = greedy_policy_from_returns_tbl(RETURNS_TBL)\n",
    "policy0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_policy(policy):\n",
    "    direction_repr = {'n':' 🡑 ', 'e':' 🡒 ', 'w':' 🡐 ', 's':' 🡓 ', None:' ⬤ '}\n",
    "\n",
    "    for row in range(foolsball.n_rows):\n",
    "        for col in range(foolsball.n_cols):\n",
    "            state = foolsball.__to_state__(row, col)\n",
    "            print(direction_repr[policy[state]],end='')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 🡒  🡓  ⬤  🡐 \n",
      " 🡒  🡒  🡓  ⬤ \n",
      " 🡓  ⬤  🡓  🡐 \n",
      " 🡒  🡒  🡓  ⬤ \n",
      " 🡑  ⬤  🡒  ⬤ \n"
     ]
    }
   ],
   "source": [
    "pretty_print_policy(policy0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with incomplete Knowledge of the environment\n",
    "- We may not know all the states of the environment in advance\n",
    "- We may not know the single-step dynamics $state_{t+1} | (state_{t},\\ action_{t})$\n",
    "- In terms of code, we cannot use the private method : `__get_next_state_on_action__()` ,as in \n",
    "\n",
    "```{python}\n",
    "next_state = foolsball.__get_next_state_on_action__(state, action)\n",
    "```\n",
    "\n",
    "- We can attempt to learn these unknows from experience(sampling). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Todo\n",
    "- Implement the function `collect_random_episode()` and run the code in the next cell to collect and print a random episode.\n",
    "\n",
    "    - The episode starts with the environment in the initial state\n",
    "    - The agent tries random actions\n",
    "    - The episode terminates when the agent collides with an opponent or reaches the goalpost.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def collect_random_episode():\n",
    "    state = foolsball.reset()\n",
    "    done = False\n",
    "    episode = []\n",
    "    \n",
    "    while not done:\n",
    "        action = np.random.choice(foolsball.actions)\n",
    "        next_state, reward, done = foolsball.step(action)\n",
    "        episode.append([state, action, reward])\n",
    "        state = next_state\n",
    "        \n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  +    +   👕    +  \n",
      "\n",
      "  +    +    +   👕  \n",
      "\n",
      "  +    ❗    +    +  \n",
      "\n",
      "  +    +    +   👕  \n",
      "\n",
      "  +   👕    +    🥅  \n",
      "\n",
      "\n",
      "[[0, 's', 0], [4, 's', 0], [8, 'w', -1], [8, 'w', -1], [8, 'n', 0], [4, 'n', 0], [0, 'e', 0], [1, 's', 0], [5, 's', -5]]\n"
     ]
    }
   ],
   "source": [
    "ep = collect_random_episode()\n",
    "foolsball.render()\n",
    "print(ep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 10: Implement discounted returns for episodes\n",
    "\n",
    "- Complete the function `discounted_return_from_episode()` that computes the discounted return for every (state,action) pair in an episode.\n",
    "- If an episode is: (s1,a1,r1),(s2,a2,r2),(s3,a3,r3), (s4),  s4 being a terminal state:\n",
    "  - The (discounted) return for (s1,a1) is r1+γ∗r2+γ2∗r3\n",
    "  - The (discounted) return for (s2, a2)is r2+γ∗r3\n",
    "  - The (discounted) return for (s3,a3) is r3\n",
    "\n",
    "- Run the next cell to print discounted returns for entire episodes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_return_from_episode(ep, gamma=0):\n",
    "    states, actions, rewards = list(zip(*ep))\n",
    "    rewards = np.asarray(rewards)\n",
    "    discount_coeffs = np.asarray([np.power(gamma,p) for p in range(len(rewards))])\n",
    "    \n",
    "    l = len(rewards)\n",
    "    discounted_returns = [np.dot(rewards[i:],discount_coeffs[:l-i]) for i in range(l)]\n",
    "    \n",
    "    return (states, actions, discounted_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0, 4, 8, 8, 8, 4, 0, 1, 5),\n",
       " ('s', 's', 'w', 'w', 'n', 'n', 'e', 's', 's'),\n",
       " [-3.6913360500000008,\n",
       "  -4.101484500000001,\n",
       "  -4.557205,\n",
       "  -3.9524500000000002,\n",
       "  -3.2805,\n",
       "  -3.6450000000000005,\n",
       "  -4.050000000000001,\n",
       "  -4.5,\n",
       "  -5.0])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discounted_return_from_episode(ep, gamma=HYPER_PARAMS['gamma'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 11: Estimate returns by simulating lots of episodes.\n",
    "\n",
    "- The code below creates two tables:\n",
    "  - ESTIMATED_RETURNS_TBL for accumulating the return for every (state,action) pair across all episodes.\n",
    "  - VISITS_COUNTS_TBL for storing the number of times a (state,action) pair appears across all episodes.\n",
    "\n",
    "- It then runs an algorithm to generate episodes and fill in these tables.\n",
    "\n",
    "Here's the idea:\n",
    "\n",
    "- Generate many random episodes\n",
    "  - Examine each (state, action) pair in every episode.\n",
    "  - Calculate and accumulate the return for this pair\n",
    "     - Since we have the full episode, we can \"see the future\" and calculate the return.\n",
    "     - The return for a (state,action) pair is just a (very bad) estimate of the \"real\" return, since we are looking at just one of the many paths that could possible contain the (state,action).\n",
    "  - Record the visit count of the (state, action) pair.\n",
    "\n",
    "- At the end the we divide the accumulated returns by the visit counts to get an averaged estimate of the retruns.\n",
    "\n",
    "## Todo:\n",
    "\n",
    "- Complete the code in the next cell to implement what's known as **Monte Carlo estimation**.\n",
    "- Run the cells to see how well the algorithm fares.\n",
    "- Does the algorithm generate sensible looking returns (estimates)?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ESTIMATED_RETURNS_TBL = pd.DataFrame.from_dict({s:{a:0 for a in foolsball.actions} for s in range(foolsball.n_states)}, orient='index')\n",
    "VISITS_COUNTS_TBL = pd.DataFrame.from_dict({s:{a:0 for a in foolsball.actions} for s in range(foolsball.n_states)}, orient='index')\n",
    "\n",
    "n_episodes = 100  #Try 100, 500, 2000\n",
    "\n",
    "for i in range(n_episodes):\n",
    "    episode_i = collect_random_episode()\n",
    "    states, actions, discounted_returns = discounted_return_from_episode(episode_i, gamma=HYPER_PARAMS['gamma'])\n",
    "\n",
    "    for s,a,ret in zip(states, actions, discounted_returns):\n",
    "        ESTIMATED_RETURNS_TBL.loc[s,a] += ret\n",
    "        VISITS_COUNTS_TBL.loc[s,a] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>e</th>\n",
       "      <th>w</th>\n",
       "      <th>s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-4.612949</td>\n",
       "      <td>-3.974429</td>\n",
       "      <td>-4.711919</td>\n",
       "      <td>-3.449721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-5.172970</td>\n",
       "      <td>-4.864865</td>\n",
       "      <td>-3.777823</td>\n",
       "      <td>-3.602279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-3.646191</td>\n",
       "      <td>-3.647746</td>\n",
       "      <td>-4.461940</td>\n",
       "      <td>-3.760688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-3.519654</td>\n",
       "      <td>-3.677943</td>\n",
       "      <td>-3.448259</td>\n",
       "      <td>-4.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-4.500000</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>-2.974882</td>\n",
       "      <td>-2.525964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-3.489774</td>\n",
       "      <td>-4.722222</td>\n",
       "      <td>-4.598359</td>\n",
       "      <td>-3.746958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-1.623560</td>\n",
       "      <td>-3.054150</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-2.500000</td>\n",
       "      <td>-3.275167</td>\n",
       "      <td>-2.025000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-3.756508</td>\n",
       "      <td>-3.774375</td>\n",
       "      <td>-4.095687</td>\n",
       "      <td>-3.812054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-2.500000</td>\n",
       "      <td>-2.250000</td>\n",
       "      <td>-2.025000</td>\n",
       "      <td>-4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-3.446345</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>-2.865817</td>\n",
       "      <td>-4.202243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           n         e         w         s\n",
       "0  -4.612949 -3.974429 -4.711919 -3.449721\n",
       "1  -5.172970 -4.864865 -3.777823 -3.602279\n",
       "2   0.000000  0.000000  0.000000  0.000000\n",
       "3   0.000000  0.000000  0.000000  0.000000\n",
       "4  -3.646191 -3.647746 -4.461940 -3.760688\n",
       "5  -3.519654 -3.677943 -3.448259 -4.800000\n",
       "6  -4.500000 -3.750000 -2.974882 -2.525964\n",
       "7   0.000000  0.000000  0.000000  0.000000\n",
       "8  -3.489774 -4.722222 -4.598359 -3.746958\n",
       "9   0.000000  0.000000  0.000000  0.000000\n",
       "10 -1.623560 -3.054150  0.000000 -2.250000\n",
       "11 -2.500000 -3.275167 -2.025000  0.000000\n",
       "12 -3.756508 -3.774375 -4.095687 -3.812054\n",
       "13 -2.500000 -2.250000 -2.025000 -4.000000\n",
       "14  0.000000 -3.333333  0.000000  0.000000\n",
       "15  0.000000  0.000000  0.000000  0.000000\n",
       "16 -3.446345 -3.750000 -2.865817 -4.202243\n",
       "17  0.000000  0.000000  0.000000  0.000000\n",
       "18  0.000000  0.000000  0.000000  0.000000\n",
       "19  0.000000  0.000000  0.000000  0.000000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimated_returns = ESTIMATED_RETURNS_TBL.div(VISITS_COUNTS_TBL+1) ## Averaging returns. Avoid dividing by zeros.\n",
    "estimated_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 🡓  🡓  ⬤  🡑 \n",
      " 🡑  🡐  🡓  ⬤ \n",
      " 🡑  ⬤  🡐  🡓 \n",
      " 🡑  🡐  🡑  ⬤ \n",
      " 🡐  ⬤  🡑  ⬤ \n"
     ]
    }
   ],
   "source": [
    "policy1 = greedy_policy_from_returns_tbl(estimated_returns)\n",
    "pretty_print_policy(policy1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 12: Exploiting the information in the returns table.\n",
    "\n",
    "- We are improving our estimates of the returns with each successive episode.\n",
    "- But we are still generating random episodes throughout.\n",
    "- We should also exploit the information we accrue in the RETURNS table.\n",
    "\n",
    "- The implementation below is quite similar to collect_random_episode() but here's the key difference:\n",
    "   - In state s, collect_random_episode() was returning a random action from ('n', 's', 'e', 'w').\n",
    "   - But from the returns table we know that one of the action, say 'e' generates the best returns so we can make a greedy choice and always return 'e'.\n",
    "   - This is what collect_greedy_episode_from_returns_tbl() does.\n",
    "\n",
    "- Run the next to cells to see the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_greedy_episode_from_returns_tbl(table, max_ep_len=20):\n",
    "    episode = []\n",
    "    state = foolsball.reset()\n",
    "    done = False\n",
    "  \n",
    "    for _ in range(max_ep_len):\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        greedy_action = table.loc[state].idxmax()\n",
    "        next_state, reward, done = foolsball.step(greedy_action)\n",
    "        episode.append([state, greedy_action, reward])\n",
    "        state = next_state\n",
    "        \n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 's', 0],\n",
       " [4, 'n', 0],\n",
       " [0, 's', 0],\n",
       " [4, 'n', 0],\n",
       " [0, 's', 0],\n",
       " [4, 'n', 0],\n",
       " [0, 's', 0],\n",
       " [4, 'n', 0],\n",
       " [0, 's', 0],\n",
       " [4, 'n', 0],\n",
       " [0, 's', 0],\n",
       " [4, 'n', 0],\n",
       " [0, 's', 0],\n",
       " [4, 'n', 0],\n",
       " [0, 's', 0],\n",
       " [4, 'n', 0],\n",
       " [0, 's', 0],\n",
       " [4, 'n', 0],\n",
       " [0, 's', 0],\n",
       " [4, 'n', 0]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collect_greedy_episode_from_returns_tbl(estimated_returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 13: Updating the Returns Table Using Greedy Episodes\n",
    "## Todo\n",
    "\n",
    "- Implement the loop in the cell below to update the returns table.\n",
    "\n",
    "- The code will be exactly what we used earlier, except that it will use greedy episodes.\n",
    "\n",
    "- Run the next few cells to evaluate the effectiveness.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ESTIMATED_RETURNS_TBL = pd.DataFrame.from_dict({s:{a:0 for a in foolsball.actions} for s in range(foolsball.n_states)}, orient='index')\n",
    "VISITS_COUNTS_TBL = pd.DataFrame.from_dict({s:{a:0 for a in foolsball.actions} for s in range(foolsball.n_states)}, orient='index')\n",
    "\n",
    "n_episodes = 1000\n",
    "\n",
    "for i in range(n_episodes):\n",
    "    estimated_returns = ESTIMATED_RETURNS_TBL.div(VISITS_COUNTS_TBL+1)\n",
    "    episode_i = collect_greedy_episode_from_returns_tbl(estimated_returns)\n",
    "    \n",
    "    states, actions, discounted_returns = discounted_return_from_episode(episode_i, gamma=HYPER_PARAMS['gamma'])\n",
    "    \n",
    "    for s,a,ret in zip(states, actions, discounted_returns):\n",
    "        ESTIMATED_RETURNS_TBL.loc[s,a] += ret\n",
    "        VISITS_COUNTS_TBL.loc[s,a] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>e</th>\n",
       "      <th>w</th>\n",
       "      <th>s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-5.759138</td>\n",
       "      <td>-3.892117</td>\n",
       "      <td>-5.759138</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-5.607883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           n         e         w    s\n",
       "0  -5.759138 -3.892117 -5.759138  0.0\n",
       "1  -5.607883  0.000000  0.000000  0.0\n",
       "2   0.000000  0.000000  0.000000  0.0\n",
       "3   0.000000  0.000000  0.000000  0.0\n",
       "4   0.000000  0.000000  0.000000  0.0\n",
       "5   0.000000  0.000000  0.000000  0.0\n",
       "6   0.000000  0.000000  0.000000  0.0\n",
       "7   0.000000  0.000000  0.000000  0.0\n",
       "8   0.000000  0.000000  0.000000  0.0\n",
       "9   0.000000  0.000000  0.000000  0.0\n",
       "10  0.000000  0.000000  0.000000  0.0\n",
       "11  0.000000  0.000000  0.000000  0.0\n",
       "12  0.000000  0.000000  0.000000  0.0\n",
       "13  0.000000  0.000000  0.000000  0.0\n",
       "14  0.000000  0.000000  0.000000  0.0\n",
       "15  0.000000  0.000000  0.000000  0.0\n",
       "16  0.000000  0.000000  0.000000  0.0\n",
       "17  0.000000  0.000000  0.000000  0.0\n",
       "18  0.000000  0.000000  0.000000  0.0\n",
       "19  0.000000  0.000000  0.000000  0.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimated_returns = ESTIMATED_RETURNS_TBL.div(VISITS_COUNTS_TBL+1) ## Averaging returns. Avoid dividing by zeros.\n",
    "estimated_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy2 = greedy_policy_from_returns_tbl(estimated_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 🡓  🡒  ⬤  🡑 \n",
      " 🡑  🡑  🡑  ⬤ \n",
      " 🡑  ⬤  🡑  🡑 \n",
      " 🡑  🡑  🡑  ⬤ \n",
      " 🡑  ⬤  🡑  ⬤ \n"
     ]
    }
   ],
   "source": [
    "pretty_print_policy(policy2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 14: The Exploration-exploitation Dilemma¶\n",
    "\n",
    "- We have tried pure exploration (with random episodes)\n",
    "- We have also tried pure exploitation (with policy generated from the returns table)\n",
    "- A good agent should try to balance both.\n",
    "\n",
    "## Epsilon-greedy episodes\n",
    "\n",
    "- An epsilon greedy episode blends the previous two approaches\n",
    "\n",
    "- Precisely, when in state s:\n",
    "  - The epsilon greedy episode will pick the action yielding the highest returns with a high probability, say 0.8\n",
    "  - With a low probability it can return one of the suboptimal, random actions.\n",
    "  - The hyperparameter epsilon or ϵ decides the probability of selecting a random action and 1-epsilon is the probability of picking the best action.\n",
    "\n",
    "  - Example with epsilon = 0.2 and assuming w is the best action\n",
    "    - state s\n",
    "    - Actions = ('n','e','w','s')\n",
    "    - Best action (yielding highest return) = 'w'\n",
    "    - Sampling probabilities = [ϵ4,ϵ4,1−ϵ+ϵ4,ϵ4]=[0.05,0.05,0.85,0.05]\n",
    "\n",
    "## Todo:\n",
    "\n",
    "Finish the code below and look at how the output differs from the other two methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_epsilon_greedy_episode_from_returns_tbl(table, max_ep_len=20, epsilon=0.1):\n",
    "    state = foolsball.reset()\n",
    "    done = False\n",
    "    episode = []\n",
    "    \n",
    "    for _ in range(max_ep_len):\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "        actions = table.columns\n",
    "        action_probs = np.asarray([epsilon/len(actions)]*len(actions),dtype=np.float)\n",
    "\n",
    "        greedy_action_index = table.loc[state].argmax()\n",
    "        action_probs[greedy_action_index] += 1-epsilon\n",
    "\n",
    "        epsilon_greedy_action = #Todo: use np.random.choice() to sample epsilon-greedily\n",
    "\n",
    "        next_state, reward, done = foolsball.step(epsilon_greedy_action)\n",
    "        episode.append([state, epsilon_greedy_action, reward])\n",
    "        state = next_state\n",
    "\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate an epsilon-greedy episode every time. \n",
    "collect_epsilon_greedy_episode_from_returns_tbl(estimated_returns, epsilon=1)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Rediscovering_RL_Notebook_0_SOLVED.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
